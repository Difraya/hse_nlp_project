{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ноутбук с экспериментами"
      ],
      "metadata": {
        "id": "s9l16J3aIM_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Вывод"
      ],
      "metadata": {
        "id": "NnZA1jMRbPsw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сразу кратко опишу все проведенные мной эксперименты, чтобы не приходилось искать выводы в конце или далее по тексту."
      ],
      "metadata": {
        "id": "QFCnkv5zbVbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сначала, я создал один большой вектор из всех числовых признаков, созданных мной ранее различный статистик текстовых данных, которые описаны ниже. Также я возлагал надежды на векторы полученные из частотных словарей см. в описаниях признаков.<br>\n",
        "Я предсказывал классы через косинусную близость полученных векторов.<br>\n",
        "Затем попробовал уменьшить размерности PCA.<br>\n",
        "Затем я стал пробовать на этих векторах линейные модели в пайплайнах с различными скалерами и гиперпараметрами.<br>\n",
        "Затем я попробовал обучать на BoW и Tfidf.<br>\n",
        "С Tfidf модели стали показывать лучшие результаты и далее я стал экспериментировать уже с этими данными, пробовал объединять с полученным в начале вектором, но не получил прироста. <br>\n",
        "Лучшую модель я получил в итоге с пайплайном TfidfVectorizer(ngram_range=(1, 2)) с OneVsRestClassifier, MaxAbsScaler и LogisticRegression.<br>\n",
        "Далее я пробовал этот же пайплайн но с ngram_range=(1, 1) и MultinomialNB, но получил результат похуже.<br>\n",
        "Далее я пробовал стекинг лучших моделей LR и NB c BoW и Tfidf, результат не улучшился. StackingClassifier не работал на классах с одним объектом, пришлось его перенастраивать. <br>\n",
        "Я пробовал сложить вероятности классов лучших моделей и усреднить, но это не помогло.  <br>\n",
        "Затем, перейдя на Kaggle я попробовал обучать на векторах SentenceTransformer, TinyBert и Word2Vec, модели LR и Catboost с использованием GPU, на этих признаках по отдельности, вместе и с различными комбинациями (с включениями TFidf и вектора на статистиках) но эти модели не показали более высокий результат. <br>\n",
        "Пробовал отобрать наиболее значимые признаки при помощи RFECV, но не вышло, из-за недостаточной памяти. <br>\n",
        "Возможно, что результат удастся улучшить, если попробовать дообучить лучшую модель на каких-то векторах из этих экспериментов. Или использовать ансамбль из Catboost с векторами и лучшей модели."
      ],
      "metadata": {
        "id": "rvoC2V3wbhpp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rwftSE1HoLX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwXG6GesHwU5",
        "outputId": "888c0567-6446-42e2-d44b-2951fb362a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ссылки на файлы с данными:<br>\n",
        "[df_train.pq](https://drive.google.com/file/d/1-8MfMUBhrec_Nqsh_4NFYhtoeY3hZg1z/view?usp=sharing)<br>\n",
        "[df_test.pq](https://drive.google.com/file/d/1vMGRqYBuR8PD6owZg1UACS4Ou_biPTZt/view?usp=sharing)"
      ],
      "metadata": {
        "id": "4irBR3nrIbo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_parquet('//content//drive//MyDrive//AI//NLP//df_train.pq')\n",
        "df_test = pd.read_parquet('//content//drive//MyDrive//AI//NLP//df_test.pq')"
      ],
      "metadata": {
        "id": "bFVxxrb7H_Pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Описание признаков тренировочного и тестового датасетов\n",
        "1.  'author' - имя и фамилия автора **(целевая переменная 100 классов)**\n",
        "2.  'text_' - оригинальный полный текст книги без пробельных символов и метаданных\n",
        "3.  'cnt_sent' - количество предложений\n",
        "4.  'text_len' - количество символов в оригинальном тексте, до обрезки\n",
        "5.  'text' - обрезанный в середине текст книги до максимальной длинны в 200000 слов\n",
        "6.  'text_len2' - количество символов в \"text\"\n",
        "7.  'words_cnt' - количество слов в обрезанном тексте\n",
        "8.  'wrds_sent_cnt' - отношение количества слов к количеству предложений, или средняя длина предложения\n",
        "9.  'cnt_words_unique' - количество уникальных слов\n",
        "10. 'unwords_words' - отношение количества уникальных слов к количеству слов\n",
        "11. 'median_word_length' - медианная длина слов\n",
        "12. 'mean_word_length' - средняя длина слова\n",
        "13. 'max_word_length' - максимальная длина слова\n",
        "14. 'words_symbols' - отношение количества слов к количеству символов\n",
        "15. 'words_dots' - отношение количества слов к количеству точек\n",
        "16. 'words_commas' - отношение количества слов к количеству запятых\n",
        "17. 'words_excls' - отношение количества слов к количеству восклицательных знаков\n",
        "18. 'words_questions' - отношение количества слов к количеству вопросительных знаков\n",
        "19. 'words_semicolons' - отношение количества слов к количеству точек с запятой\n",
        "20. 'words_colons' - отношение количества слов к количеству двоеточий\n",
        "21. 'words_dashs' - отношение количества слов к количеству тире\n",
        "22. 'words_aposts' - отношение количества слов к количеству апострофов\n",
        "23. 'words_ellipsis' - отношение количества слов к количеству многоточий\n",
        "24. 'words_quots' - отношение количества слов к количеству кавычек\n",
        "25. 'cnt_adv_freq' - словарь с количествами частотных наречий\n",
        "26. 'cnt_swadesh_freq' - словарь с количествами слов из списка Сводеша\n",
        "27. 'cnt_word_eng' - количество уникальных слов из словаря англ. языка\n",
        "28. 'prc_wrds_not_eng' - отношение количества английских слов к количеству слов\n",
        "29. 'uniq_word_cnt' - словарь с количествами уникальных слов\n",
        "30. 'cnt_punct_frq' - словарь с количествами знаков пунктуации\n",
        "31. 'lex_div' - (lexical_diversity) - лексическое разнообразие\n",
        "32. 'tfidf_keywords' - ключевые слова с максимальными TF-IDF значениями\n",
        "33. 'pos_frq' - словарь с количествами слов по частям речи\n",
        "34. 'pos_cnt' - количество уникальных частей речи\n",
        "35. 'ent_frq' - словарь с количествами слов именнованных сущностей\n",
        "36. 'ent_cnt' - количество уникальных именнованных сущностей\n",
        "37. 'uchars_frq' - словарь с количествами букв английского алфавита\n",
        "38. 'uchars_cnt' - количество уникальных букв английского алфавита\n",
        "39. 'fk_score' - показатель уровня читаемости текста по формуле Flesch-Kincaid"
      ],
      "metadata": {
        "id": "mc9UbhrCIG-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на статистики числовых данных"
      ],
      "metadata": {
        "id": "N4fNboFbJfpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "6r5fBdFfJCqn",
        "outputId": "84a0bac9-dfe3-4147-fd87-4fddeb40a18a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           cnt_sent      text_len     text_len2      words_cnt  wrds_sent_cnt  \\\n",
              "count    100.000000  1.000000e+02  1.000000e+02     100.000000     100.000000   \n",
              "mean    6708.100000  5.931244e+05  5.717292e+05  106611.370000      17.625164   \n",
              "std     5023.105853  4.007281e+05  3.867990e+05   71497.536884       8.258601   \n",
              "min      142.000000  1.089200e+04  1.055000e+04    2036.000000       6.748858   \n",
              "25%     2919.250000  2.144975e+05  2.062805e+05   38724.250000      12.528176   \n",
              "50%     5438.500000  5.450980e+05  5.294670e+05  100340.500000      15.098669   \n",
              "75%     9811.250000  9.919358e+05  9.493388e+05  184172.250000      20.497069   \n",
              "max    19644.000000  1.227145e+06  1.184651e+06  199998.000000      56.632507   \n",
              "\n",
              "       cnt_words_unique  unwords_words  median_word_length  mean_word_length  \\\n",
              "count        100.000000     100.000000               100.0        100.000000   \n",
              "mean       10367.320000       0.137389                 4.0          4.343600   \n",
              "std         5874.831163       0.086244                 0.0          0.166706   \n",
              "min          761.000000       0.053816                 4.0          3.990000   \n",
              "25%         5885.250000       0.083371                 4.0          4.250000   \n",
              "50%         9967.500000       0.110327                 4.0          4.340000   \n",
              "75%        14322.000000       0.145839                 4.0          4.450000   \n",
              "max        28415.000000       0.505560                 4.0          4.930000   \n",
              "\n",
              "       max_word_length  ...  words_aposts  words_ellipsis  words_quots  \\\n",
              "count       100.000000  ...    100.000000      100.000000   100.000000   \n",
              "mean         28.380000  ...           inf             inf          inf   \n",
              "std          19.324857  ...           NaN             NaN          NaN   \n",
              "min          13.000000  ...     14.140523       61.212629    12.508128   \n",
              "25%          20.000000  ...     43.932483     2388.984091    41.958527   \n",
              "50%          23.000000  ...     78.081368             NaN   591.032145   \n",
              "75%          30.250000  ...           NaN             NaN          NaN   \n",
              "max         187.000000  ...           inf             inf          inf   \n",
              "\n",
              "       cnt_word_eng  prc_wrds_not_eng     lex_div     pos_cnt     ent_cnt  \\\n",
              "count    100.000000        100.000000  100.000000  100.000000  100.000000   \n",
              "mean    5530.500000          0.564276    0.136749   33.030000   16.160000   \n",
              "std     2678.349641          0.074756    0.086098    1.158674    1.587324   \n",
              "min      526.000000          0.347497    0.053738   27.000000   11.000000   \n",
              "25%     3544.250000          0.504767    0.084104   33.000000   15.000000   \n",
              "50%     5706.500000          0.569669    0.108338   33.000000   17.000000   \n",
              "75%     7500.000000          0.618218    0.145886   34.000000   17.000000   \n",
              "max    13685.000000          0.699397    0.509004   34.000000   18.000000   \n",
              "\n",
              "       uchars_cnt    fk_score  \n",
              "count  100.000000  100.000000  \n",
              "mean    51.380000    7.331000  \n",
              "std      1.012847    3.428304  \n",
              "min     46.000000    1.800000  \n",
              "25%     51.000000    5.250000  \n",
              "50%     52.000000    6.650000  \n",
              "75%     52.000000    8.300000  \n",
              "max     52.000000   22.100000  \n",
              "\n",
              "[8 rows x 28 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8e7d182c-f257-40c2-a116-f09a532ae008\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cnt_sent</th>\n",
              "      <th>text_len</th>\n",
              "      <th>text_len2</th>\n",
              "      <th>words_cnt</th>\n",
              "      <th>wrds_sent_cnt</th>\n",
              "      <th>cnt_words_unique</th>\n",
              "      <th>unwords_words</th>\n",
              "      <th>median_word_length</th>\n",
              "      <th>mean_word_length</th>\n",
              "      <th>max_word_length</th>\n",
              "      <th>...</th>\n",
              "      <th>words_aposts</th>\n",
              "      <th>words_ellipsis</th>\n",
              "      <th>words_quots</th>\n",
              "      <th>cnt_word_eng</th>\n",
              "      <th>prc_wrds_not_eng</th>\n",
              "      <th>lex_div</th>\n",
              "      <th>pos_cnt</th>\n",
              "      <th>ent_cnt</th>\n",
              "      <th>uchars_cnt</th>\n",
              "      <th>fk_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>100.000000</td>\n",
              "      <td>1.000000e+02</td>\n",
              "      <td>1.000000e+02</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>100.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>6708.100000</td>\n",
              "      <td>5.931244e+05</td>\n",
              "      <td>5.717292e+05</td>\n",
              "      <td>106611.370000</td>\n",
              "      <td>17.625164</td>\n",
              "      <td>10367.320000</td>\n",
              "      <td>0.137389</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.343600</td>\n",
              "      <td>28.380000</td>\n",
              "      <td>...</td>\n",
              "      <td>inf</td>\n",
              "      <td>inf</td>\n",
              "      <td>inf</td>\n",
              "      <td>5530.500000</td>\n",
              "      <td>0.564276</td>\n",
              "      <td>0.136749</td>\n",
              "      <td>33.030000</td>\n",
              "      <td>16.160000</td>\n",
              "      <td>51.380000</td>\n",
              "      <td>7.331000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>5023.105853</td>\n",
              "      <td>4.007281e+05</td>\n",
              "      <td>3.867990e+05</td>\n",
              "      <td>71497.536884</td>\n",
              "      <td>8.258601</td>\n",
              "      <td>5874.831163</td>\n",
              "      <td>0.086244</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.166706</td>\n",
              "      <td>19.324857</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2678.349641</td>\n",
              "      <td>0.074756</td>\n",
              "      <td>0.086098</td>\n",
              "      <td>1.158674</td>\n",
              "      <td>1.587324</td>\n",
              "      <td>1.012847</td>\n",
              "      <td>3.428304</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>142.000000</td>\n",
              "      <td>1.089200e+04</td>\n",
              "      <td>1.055000e+04</td>\n",
              "      <td>2036.000000</td>\n",
              "      <td>6.748858</td>\n",
              "      <td>761.000000</td>\n",
              "      <td>0.053816</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.990000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>14.140523</td>\n",
              "      <td>61.212629</td>\n",
              "      <td>12.508128</td>\n",
              "      <td>526.000000</td>\n",
              "      <td>0.347497</td>\n",
              "      <td>0.053738</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>1.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2919.250000</td>\n",
              "      <td>2.144975e+05</td>\n",
              "      <td>2.062805e+05</td>\n",
              "      <td>38724.250000</td>\n",
              "      <td>12.528176</td>\n",
              "      <td>5885.250000</td>\n",
              "      <td>0.083371</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.250000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>43.932483</td>\n",
              "      <td>2388.984091</td>\n",
              "      <td>41.958527</td>\n",
              "      <td>3544.250000</td>\n",
              "      <td>0.504767</td>\n",
              "      <td>0.084104</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>15.000000</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>5.250000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>5438.500000</td>\n",
              "      <td>5.450980e+05</td>\n",
              "      <td>5.294670e+05</td>\n",
              "      <td>100340.500000</td>\n",
              "      <td>15.098669</td>\n",
              "      <td>9967.500000</td>\n",
              "      <td>0.110327</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.340000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>78.081368</td>\n",
              "      <td>NaN</td>\n",
              "      <td>591.032145</td>\n",
              "      <td>5706.500000</td>\n",
              "      <td>0.569669</td>\n",
              "      <td>0.108338</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>6.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>9811.250000</td>\n",
              "      <td>9.919358e+05</td>\n",
              "      <td>9.493388e+05</td>\n",
              "      <td>184172.250000</td>\n",
              "      <td>20.497069</td>\n",
              "      <td>14322.000000</td>\n",
              "      <td>0.145839</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.450000</td>\n",
              "      <td>30.250000</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7500.000000</td>\n",
              "      <td>0.618218</td>\n",
              "      <td>0.145886</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>8.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>19644.000000</td>\n",
              "      <td>1.227145e+06</td>\n",
              "      <td>1.184651e+06</td>\n",
              "      <td>199998.000000</td>\n",
              "      <td>56.632507</td>\n",
              "      <td>28415.000000</td>\n",
              "      <td>0.505560</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.930000</td>\n",
              "      <td>187.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>inf</td>\n",
              "      <td>inf</td>\n",
              "      <td>inf</td>\n",
              "      <td>13685.000000</td>\n",
              "      <td>0.699397</td>\n",
              "      <td>0.509004</td>\n",
              "      <td>34.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>22.100000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 28 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e7d182c-f257-40c2-a116-f09a532ae008')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8e7d182c-f257-40c2-a116-f09a532ae008 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8e7d182c-f257-40c2-a116-f09a532ae008');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-551397df-506f-41e3-8196-18dc28108d5c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-551397df-506f-41e3-8196-18dc28108d5c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-551397df-506f-41e3-8196-18dc28108d5c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим типы данных"
      ],
      "metadata": {
        "id": "nFsyayCPJlFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CrdZeiScJF-Q",
        "outputId": "fc639656-3664-4900-ccca-0f72fdc719bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100 entries, 0 to 99\n",
            "Data columns (total 39 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   author              100 non-null    object \n",
            " 1   text_               100 non-null    object \n",
            " 2   cnt_sent            100 non-null    int64  \n",
            " 3   text_len            100 non-null    int64  \n",
            " 4   text                100 non-null    object \n",
            " 5   text_len2           100 non-null    int64  \n",
            " 6   words_cnt           100 non-null    int64  \n",
            " 7   wrds_sent_cnt       100 non-null    float64\n",
            " 8   cnt_words_unique    100 non-null    int64  \n",
            " 9   unwords_words       100 non-null    float64\n",
            " 10  median_word_length  100 non-null    float64\n",
            " 11  mean_word_length    100 non-null    float64\n",
            " 12  max_word_length     100 non-null    int64  \n",
            " 13  words_symbols       100 non-null    float64\n",
            " 14  words_dots          100 non-null    float64\n",
            " 15  words_commas        100 non-null    float64\n",
            " 16  words_excls         100 non-null    float64\n",
            " 17  words_questions     100 non-null    float64\n",
            " 18  words_semicolons    100 non-null    float64\n",
            " 19  words_colons        100 non-null    float64\n",
            " 20  words_dashs         100 non-null    float64\n",
            " 21  words_aposts        100 non-null    float64\n",
            " 22  words_ellipsis      100 non-null    float64\n",
            " 23  words_quots         100 non-null    float64\n",
            " 24  cnt_adv_freq        100 non-null    object \n",
            " 25  cnt_swadesh_freq    100 non-null    object \n",
            " 26  cnt_word_eng        100 non-null    int64  \n",
            " 27  prc_wrds_not_eng    100 non-null    float64\n",
            " 28  uniq_word_cnt       100 non-null    object \n",
            " 29  cnt_punct_frq       100 non-null    object \n",
            " 30  lex_div             100 non-null    float64\n",
            " 31  tfidf_keywords      100 non-null    object \n",
            " 32  pos_frq             100 non-null    object \n",
            " 33  pos_cnt             100 non-null    int64  \n",
            " 34  ent_frq             100 non-null    object \n",
            " 35  ent_cnt             100 non-null    int64  \n",
            " 36  uchars_frq          100 non-null    object \n",
            " 37  uchars_cnt          100 non-null    int64  \n",
            " 38  fk_score            100 non-null    float64\n",
            "dtypes: float64(18), int64(10), object(11)\n",
            "memory usage: 30.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выполним замену типов данных float64 в flaot16 для экономии памяти и времени обучения моделей"
      ],
      "metadata": {
        "id": "7Pd0A1egJt5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_float64_to_16(df):\n",
        "    for column in df.columns:\n",
        "        if df[column].dtype == 'float64':\n",
        "            df[column] = df[column].astype('float16')\n",
        "    return df\n",
        "\n",
        "\n",
        "df_train = convert_float64_to_16(df_train)\n",
        "df_test = convert_float64_to_16(df_test)\n",
        "df_test.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_K8bnLAU9rY",
        "outputId": "b98ef1a2-094d-4bdc-f935-22600e73e3a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 51 entries, 0 to 50\n",
            "Data columns (total 39 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   author              51 non-null     object \n",
            " 1   text_               51 non-null     object \n",
            " 2   cnt_sent            51 non-null     int64  \n",
            " 3   text_len            51 non-null     int64  \n",
            " 4   text                51 non-null     object \n",
            " 5   text_len2           51 non-null     int64  \n",
            " 6   words_cnt           51 non-null     int64  \n",
            " 7   wrds_sent_cnt       51 non-null     float16\n",
            " 8   cnt_words_unique    51 non-null     int64  \n",
            " 9   unwords_words       51 non-null     float16\n",
            " 10  median_word_length  51 non-null     float16\n",
            " 11  mean_word_length    51 non-null     float16\n",
            " 12  max_word_length     51 non-null     int64  \n",
            " 13  words_symbols       51 non-null     float16\n",
            " 14  words_dots          51 non-null     float16\n",
            " 15  words_commas        51 non-null     float16\n",
            " 16  words_excls         51 non-null     float16\n",
            " 17  words_questions     51 non-null     float16\n",
            " 18  words_semicolons    51 non-null     float16\n",
            " 19  words_colons        51 non-null     float16\n",
            " 20  words_dashs         51 non-null     float16\n",
            " 21  words_aposts        51 non-null     float16\n",
            " 22  words_ellipsis      51 non-null     float16\n",
            " 23  words_quots         51 non-null     float16\n",
            " 24  cnt_adv_freq        51 non-null     object \n",
            " 25  cnt_swadesh_freq    51 non-null     object \n",
            " 26  cnt_word_eng        51 non-null     int64  \n",
            " 27  prc_wrds_not_eng    51 non-null     float16\n",
            " 28  uniq_word_cnt       51 non-null     object \n",
            " 29  cnt_punct_frq       51 non-null     object \n",
            " 30  lex_div             51 non-null     float16\n",
            " 31  tfidf_keywords      0 non-null      object \n",
            " 32  pos_frq             51 non-null     object \n",
            " 33  pos_cnt             51 non-null     int64  \n",
            " 34  ent_frq             51 non-null     object \n",
            " 35  ent_cnt             51 non-null     int64  \n",
            " 36  uchars_frq          51 non-null     object \n",
            " 37  uchars_cnt          51 non-null     int64  \n",
            " 38  fk_score            51 non-null     float16\n",
            "dtypes: float16(18), int64(10), object(11)\n",
            "memory usage: 10.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем собрать один вектор для сравнения по косинусной близости и обучения моделей из всех числовых признаков и значений словарей"
      ],
      "metadata": {
        "id": "KKS6MF_0VjCr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mKrXXlwV0C6",
        "outputId": "f885fef6-0bf0-41a8-fd33-3fe2d9513a43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['author', 'text_', 'cnt_sent', 'text_len', 'text', 'text_len2',\n",
              "       'words_cnt', 'wrds_sent_cnt', 'cnt_words_unique', 'unwords_words',\n",
              "       'median_word_length', 'mean_word_length', 'max_word_length',\n",
              "       'words_symbols', 'words_dots', 'words_commas', 'words_excls',\n",
              "       'words_questions', 'words_semicolons', 'words_colons', 'words_dashs',\n",
              "       'words_aposts', 'words_ellipsis', 'words_quots', 'cnt_adv_freq',\n",
              "       'cnt_swadesh_freq', 'cnt_word_eng', 'prc_wrds_not_eng', 'uniq_word_cnt',\n",
              "       'cnt_punct_frq', 'lex_div', 'tfidf_keywords', 'pos_frq', 'pos_cnt',\n",
              "       'ent_frq', 'ent_cnt', 'uchars_frq', 'uchars_cnt', 'fk_score'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оставим только признаки с числовыми данными"
      ],
      "metadata": {
        "id": "fFH-yZ_uKB9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain = df_train[['cnt_sent', 'text_len', 'text_len2', 'words_cnt',\n",
        "                   'wrds_sent_cnt', 'cnt_words_unique', 'unwords_words',\n",
        "       'median_word_length', 'mean_word_length', 'max_word_length',\n",
        "       'words_symbols', 'words_dots', 'words_commas', 'words_excls',\n",
        "       'words_questions', 'words_semicolons', 'words_colons', 'words_dashs',\n",
        "       'words_aposts', 'words_ellipsis', 'words_quots', 'cnt_adv_freq',\n",
        "       'cnt_swadesh_freq', 'cnt_word_eng', 'prc_wrds_not_eng', 'uniq_word_cnt',\n",
        "       'cnt_punct_frq', 'lex_div', 'pos_frq', 'pos_cnt',\n",
        "       'ent_frq', 'ent_cnt', 'uchars_frq', 'uchars_cnt', 'fk_score']]"
      ],
      "metadata": {
        "id": "Nc6EwV0_V8LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Заменим признаки с частотными словарями с постоянными последовательными значений ключей, на векторы с частотными значениями их значений"
      ],
      "metadata": {
        "id": "7GI_-w52KIYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain['cnt_adv_freq_vec'] = Xtrain['cnt_adv_freq'].apply(lambda x: np.array([v if v is not None else 0 for v in x.values()]))\n",
        "Xtrain['cnt_swadesh_freq_vec'] = Xtrain['cnt_swadesh_freq'].apply(lambda x: np.array([v if v is not None else 0 for v in x.values()]))\n",
        "Xtrain['cnt_punct_frq_vec'] = Xtrain['cnt_punct_frq'].apply(lambda x: np.array([v if v is not None else 0 for v in x.values()]))\n",
        "Xtrain['pos_frq_vec'] = Xtrain['pos_frq'].apply(lambda x: np.array([v if v is not None else 0 for v in x.values()]))\n",
        "Xtrain['ent_frq_vec'] = Xtrain['ent_frq'].apply(lambda x: np.array([v if v is not None else 0 for v in x.values()]))\n",
        "Xtrain['uchars_frq_vec'] = Xtrain['uchars_frq'].apply(lambda x: np.array([v if v is not None else 0 for v in x.values()]))"
      ],
      "metadata": {
        "id": "XIEuRbv8W9wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Удалим признаки со словарями"
      ],
      "metadata": {
        "id": "MQWphqZWKeK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain = Xtrain[['cnt_sent', 'text_len', 'text_len2', 'words_cnt',\n",
        "                   'wrds_sent_cnt', 'cnt_words_unique', 'unwords_words',\n",
        "       'median_word_length', 'mean_word_length', 'max_word_length',\n",
        "       'words_symbols', 'words_dots', 'words_commas', 'words_excls',\n",
        "       'words_questions', 'words_semicolons', 'words_colons', 'words_dashs',\n",
        "       'words_aposts', 'words_ellipsis', 'words_quots',\n",
        "       'cnt_word_eng', 'prc_wrds_not_eng', 'lex_div', 'pos_cnt', 'ent_cnt',\n",
        "       'uchars_cnt', 'fk_score', 'cnt_adv_freq_vec', 'cnt_swadesh_freq_vec',\n",
        "       'cnt_punct_frq_vec', 'pos_frq_vec', 'ent_frq_vec', 'uchars_frq_vec']]"
      ],
      "metadata": {
        "id": "ycFgmG3cbhdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объеденим все числовые признаки в один общий вектор"
      ],
      "metadata": {
        "id": "ofW8My3oKnkQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns = Xtrain.select_dtypes(include=['int64', 'float16'])\n",
        "\n",
        "Xtrain['vec'] = numerical_columns.apply(lambda row: np.array(row), axis=1)"
      ],
      "metadata": {
        "id": "41fgCc5jcnxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оставим только полученные признаки с векторами"
      ],
      "metadata": {
        "id": "vlCvHsBTKwak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain = Xtrain[['vec', 'cnt_adv_freq_vec', 'cnt_swadesh_freq_vec',\n",
        "  'cnt_punct_frq_vec', 'pos_frq_vec', 'ent_frq_vec', 'uchars_frq_vec']]"
      ],
      "metadata": {
        "id": "qSRl5jlWd85J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объеденим все полученные вектора"
      ],
      "metadata": {
        "id": "JwnHadDOK3BQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combine_vectors(row):\n",
        "    arrays = [\n",
        "        row['vec'],\n",
        "        row['cnt_adv_freq_vec'],\n",
        "        row['cnt_swadesh_freq_vec'],\n",
        "        row['cnt_punct_frq_vec'],\n",
        "        row['pos_frq_vec'],\n",
        "        row['ent_frq_vec'],\n",
        "        row['uchars_frq_vec']\n",
        "    ]\n",
        "    # Определяем максимальный размер среди всех массивов\n",
        "    uniform_size = max(array.shape[0] for array in arrays)\n",
        "    # Удлиняем каждый массив до самого длинного, добавляя нули в конце\n",
        "    arrays = [np.pad(array, (0, uniform_size - array.shape[0]), 'constant') for array in arrays]\n",
        "    # Объединяем массивы\n",
        "    return np.vstack(arrays)"
      ],
      "metadata": {
        "id": "jGAoF3xBo43o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain['vector'] = Xtrain.apply(combine_vectors, axis=1)"
      ],
      "metadata": {
        "id": "PBudDidBcyZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Возьмём только полученный общий вектор"
      ],
      "metadata": {
        "id": "xsinXY8sK-4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = Xtrain['vector']"
      ],
      "metadata": {
        "id": "FF3zw6paeh-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выпоолним, те же действия для тестового датасета"
      ],
      "metadata": {
        "id": "NgoZuSW3LGse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtest = df_test[['cnt_sent', 'text_len', 'text_len2', 'words_cnt',\n",
        "                   'wrds_sent_cnt', 'cnt_words_unique', 'unwords_words',\n",
        "       'median_word_length', 'mean_word_length', 'max_word_length',\n",
        "       'words_symbols', 'words_dots', 'words_commas', 'words_excls',\n",
        "       'words_questions', 'words_semicolons', 'words_colons', 'words_dashs',\n",
        "       'words_aposts', 'words_ellipsis', 'words_quots', 'cnt_adv_freq',\n",
        "       'cnt_swadesh_freq', 'cnt_word_eng', 'prc_wrds_not_eng', 'uniq_word_cnt',\n",
        "       'cnt_punct_frq', 'lex_div', 'pos_frq', 'pos_cnt',\n",
        "       'ent_frq', 'ent_cnt', 'uchars_frq', 'uchars_cnt', 'fk_score']]"
      ],
      "metadata": {
        "id": "5NuRYSnyWgA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtest['cnt_adv_freq_vec'] = Xtest['cnt_adv_freq'].apply(lambda x: np.array([v if v is not None else 0 for v in x.values()]))\n",
        "Xtest['cnt_swadesh_freq_vec'] = Xtest['cnt_swadesh_freq'].apply(lambda x: np.array([v if v is not None else 0 for v in x.values()]))\n",
        "Xtest['cnt_punct_frq_vec'] = Xtest['cnt_punct_frq'].apply(lambda x: np.array([v if v is not None else 0 for v in x.values()]))\n",
        "Xtest['pos_frq_vec'] = Xtest['pos_frq'].apply(lambda x: np.array([v if v is not None else 0 for v in x.values()]))\n",
        "Xtest['ent_frq_vec'] = Xtest['ent_frq'].apply(lambda x: np.array([v if v is not None else 0 for v in x.values()]))\n",
        "Xtest['uchars_frq_vec'] = Xtest['uchars_frq'].apply(lambda x: np.array([v if v is not None else 0 for v in x.values()]))"
      ],
      "metadata": {
        "id": "uv2pKE0Lez6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtest = Xtest[['cnt_sent', 'text_len', 'text_len2', 'words_cnt',\n",
        "                   'wrds_sent_cnt', 'cnt_words_unique', 'unwords_words',\n",
        "       'median_word_length', 'mean_word_length', 'max_word_length',\n",
        "       'words_symbols', 'words_dots', 'words_commas', 'words_excls',\n",
        "       'words_questions', 'words_semicolons', 'words_colons', 'words_dashs',\n",
        "       'words_aposts', 'words_ellipsis', 'words_quots',\n",
        "       'cnt_word_eng', 'prc_wrds_not_eng', 'lex_div', 'pos_cnt', 'ent_cnt',\n",
        "       'uchars_cnt', 'fk_score', 'cnt_adv_freq_vec', 'cnt_swadesh_freq_vec',\n",
        "       'cnt_punct_frq_vec', 'pos_frq_vec', 'ent_frq_vec', 'uchars_frq_vec']]"
      ],
      "metadata": {
        "id": "JTDHu4CgfFXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_columns = Xtest.select_dtypes(include=['int64', 'float16'])\n",
        "\n",
        "Xtest['vec'] = numerical_columns.apply(lambda row: np.array(row), axis=1)\n",
        "Xtest = Xtest[['vec', 'cnt_adv_freq_vec', 'cnt_swadesh_freq_vec',\n",
        "  'cnt_punct_frq_vec', 'pos_frq_vec', 'ent_frq_vec', 'uchars_frq_vec']]\n",
        "Xtest['vector'] = Xtest.apply(combine_vectors, axis=1)\n",
        "X_test = Xtest['vector']"
      ],
      "metadata": {
        "id": "XQ4D4JZAfIxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Заменим бесконечности на нули и перобразуем вектор в одноуровневый"
      ],
      "metadata": {
        "id": "cQgNGFN4LMZR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inf_to_zerro(data):\n",
        "    for arr in data:\n",
        "        arr[np.isinf(arr)] = 0\n",
        "    return data\n",
        "\n",
        "\n",
        "X_train = X_train.apply(lambda x: inf_to_zerro(x))\n",
        "X_test = X_test.apply(lambda x: inf_to_zerro(x))\n",
        "X_train = X_train.apply(lambda x: np.concatenate(x))\n",
        "X_test = X_test.apply(lambda x: np.concatenate(x))"
      ],
      "metadata": {
        "id": "kDsebOHhhviE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выберем целевые метки классов для обучения и проверки моделей"
      ],
      "metadata": {
        "id": "QpQpsU1fLbco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ytrain = df_train['author']\n",
        "ytest = df_test['author']"
      ],
      "metadata": {
        "id": "lqX72p6dVV5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишем функцию для вывода результата обучения модели"
      ],
      "metadata": {
        "id": "E9gQrQlbLiq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "def calculate_multiclass_metrics(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Рассчитывает основные метрики для многоклассовой классификации.\n",
        "\n",
        "    :param y_true: Список или массив истинных значений классов.\n",
        "    :param y_pred: Список или массив предсказанных значений классов.\n",
        "    :return: Словарь с основными метриками.\n",
        "    \"\"\"\n",
        "\n",
        "    metrics = {\n",
        "        'accuracy': round(accuracy_score(y_true, y_pred), 4),\n",
        "        'precision_macro': round(precision_score(y_true, y_pred, average='macro'), 4),\n",
        "        'recall_macro': round(recall_score(y_true, y_pred, average='macro'), 4),\n",
        "        'f1_macro': round(f1_score(y_true, y_pred, average='macro'), 4),\n",
        "        'precision_micro': round(precision_score(y_true, y_pred, average='micro'), 4),\n",
        "        'recall_micro': round(recall_score(y_true, y_pred, average='micro'), 4),\n",
        "        'f1_micro': round(f1_score(y_true, y_pred, average='micro'), 4),\n",
        "    }\n",
        "\n",
        "    # Также можно вывести подробный отчет по каждому классу\n",
        "    report = classification_report(y_true, y_pred)\n",
        "    print(\"Classification Report:\\n\", report)\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "kksN-Q5gg6rD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем получить предсказания классов на основе косинусной близости векторов объектов"
      ],
      "metadata": {
        "id": "bplrG1VaLqrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Преобразуем серии в numpy массивы\n",
        "X_train_np = np.array(X_train.tolist())\n",
        "X_test_np = np.array(X_test.tolist())\n",
        "\n",
        "# Рассчитываем косинусную близость\n",
        "similarity_matrix = cosine_similarity(X_test_np, X_train_np)\n",
        "\n",
        "# Инициализируем список для предсказанных меток\n",
        "y_pred = []\n",
        "\n",
        "# Для каждого вектора в X_test находим наиболее близкий вектор в X_train\n",
        "for i in range(len(X_test_np)):\n",
        "    # Находим индекс наиболее близкого вектора в X_train\n",
        "    closest_index = np.argmax(similarity_matrix[i])\n",
        "\n",
        "    # Сохраняем соответствующую метку класса из ytrain\n",
        "    predicted_label = ytrain.iloc[closest_index]\n",
        "\n",
        "    # Добавляем метку в список предсказаний\n",
        "    y_pred.append(predicted_label)\n",
        "\n",
        "# Вызываем вашу функцию для оценки результатов\n",
        "calculate_multiclass_metrics(ytest.tolist(), y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syZcDtkFVbTW",
        "outputId": "af9071f3-7e01-4492-9325-fd7bd3d55f72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "                       Aesop       0.00      0.00      0.00         1\n",
            "             Agatha_Christie       0.00      0.00      0.00         1\n",
            "        Ahmet_Hamdi_Tanpinar       0.00      0.00      0.00         0\n",
            "               Aldous_Huxley       0.00      0.00      0.00         0\n",
            "             Alexandre_Dumas       1.00      1.00      1.00         1\n",
            "             Alphonse_Daudet       1.00      1.00      1.00         1\n",
            "               Anton_Chekhov       0.00      0.00      0.00         1\n",
            "                Aristophanes       0.00      0.00      0.00         1\n",
            "          Arthur_Conan_Doyle       0.00      0.00      0.00         1\n",
            "              Arthur_Rimbaud       0.00      0.00      0.00         0\n",
            "                Beaumarchais       0.00      0.00      0.00         0\n",
            "             Charles_Dickens       0.00      0.00      0.00         1\n",
            "             Dante_Alighieri       0.00      0.00      0.00         1\n",
            "             Edgar_Allan_Poe       0.00      0.00      0.00         1\n",
            "          F_Scott_Fitzgerald       0.00      0.00      0.00         1\n",
            "           Francois_Rabelais       0.00      0.00      0.00         0\n",
            "                 Franz_Kafka       1.00      1.00      1.00         1\n",
            "          Fyodor_Dostoyevsky       1.00      1.00      1.00         1\n",
            "            Geoffrey_Chaucer       1.00      1.00      1.00         1\n",
            "         George_Bernard_Shaw       0.00      0.00      0.00         0\n",
            "               George_Orwell       0.00      0.00      0.00         1\n",
            "          Giovanni_Boccaccio       0.00      0.00      0.00         1\n",
            "            Graf_Leo_Tolstoy       0.00      0.00      0.00         1\n",
            "           Guy_De_Maupassant       0.00      0.00      0.00         1\n",
            "               H_P_Lovecraft       0.00      0.00      0.00         1\n",
            "     Hans_Christian_Andersen       1.00      1.00      1.00         1\n",
            "             Haruki_Murakami       1.00      1.00      1.00         1\n",
            "             Herman_Melville       0.00      0.00      0.00         1\n",
            "            Honore_De_Balzac       1.00      1.00      1.00         1\n",
            "                Isaac_Asimov       0.00      0.00      0.00         1\n",
            "               Italo_Calvino       0.50      1.00      0.67         1\n",
            "               Ivan_Turgenev       0.00      0.00      0.00         1\n",
            "               J_R_R_Tolkien       0.00      0.00      0.00         1\n",
            "                 Jack_London       0.00      0.00      0.00         1\n",
            "       James_Fenimore_Cooper       0.00      0.00      0.00         1\n",
            "                 James_Joyce       0.00      0.00      0.00         1\n",
            "                 Jane_Austen       0.00      0.00      0.00         1\n",
            "         Jean_De_La_Fontaine       0.00      0.00      0.00         1\n",
            "                 Jean_Racine       0.00      0.00      0.00         0\n",
            "  Johann_Wolfgang_Von_Goethe       0.00      0.00      0.00         1\n",
            "                  John_Keats       0.00      0.00      0.00         0\n",
            "                 Jules_Verne       0.00      0.00      0.00         1\n",
            "                  Mark_Twain       0.00      0.00      0.00         1\n",
            "                 Maxim_Gorky       1.00      1.00      1.00         1\n",
            "                  Mayne_Reid       0.00      0.00      0.00         1\n",
            "Miguel_De_Cervantes_Saavedra       1.00      1.00      1.00         1\n",
            "                     Moliere       0.00      0.00      0.00         1\n",
            "                 Neil_Gaiman       0.00      0.00      0.00         0\n",
            "                     O_Henry       0.00      0.00      0.00         1\n",
            "                 Oscar_Wilde       0.00      0.00      0.00         1\n",
            "      Publius_Vergilius_Maro       0.00      0.00      0.00         1\n",
            "          Rainer_Maria_Rilke       0.00      0.00      0.00         0\n",
            "      Robert_Louis_Stevenson       0.00      0.00      0.00         1\n",
            "                   Sophocles       0.50      1.00      0.67         1\n",
            "                    Stendhal       0.00      0.00      0.00         1\n",
            "                   T_S_Eliot       0.00      0.00      0.00         0\n",
            "                 Thomas_Mann       0.00      0.00      0.00         1\n",
            "              Thomas_Pynchon       0.00      0.00      0.00         0\n",
            "                 Umberto_Eco       1.00      1.00      1.00         1\n",
            "                 Victor_Hugo       1.00      1.00      1.00         1\n",
            "              Virginia_Woolf       0.00      0.00      0.00         1\n",
            "            William_Faulkner       0.00      0.00      0.00         1\n",
            "         William_Shakespeare       0.00      0.00      0.00         1\n",
            "\n",
            "                    accuracy                           0.27        51\n",
            "                   macro avg       0.21      0.22      0.21        51\n",
            "                weighted avg       0.25      0.27      0.26        51\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.2745,\n",
              " 'precision_macro': 0.2063,\n",
              " 'recall_macro': 0.2222,\n",
              " 'f1_macro': 0.2116,\n",
              " 'precision_micro': 0.2745,\n",
              " 'recall_micro': 0.2745,\n",
              " 'f1_micro': 0.2745}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем обучить логистическую регрессию с StandardScaler"
      ],
      "metadata": {
        "id": "ZxLqJBERL2uU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Создаем конвейер для масштабирования данных и обучения модели\n",
        "model = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000))\n",
        "\n",
        "# Обучаем модель на тренировочных данных\n",
        "model.fit(X_train_np, ytrain)\n",
        "\n",
        "# Делаем предсказания на тестовых данных\n",
        "y_pred = model.predict(X_test_np)\n",
        "\n",
        "# Оцениваем результаты\n",
        "calculate_multiclass_metrics(ytest.tolist(), y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9bfIJfovUG_",
        "outputId": "161fb170-0e45-4e6c-ce49-8052d07d6874"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "                       Aesop       0.00      0.00      0.00         1\n",
            "             Agatha_Christie       0.00      0.00      0.00         1\n",
            "               Aldous_Huxley       0.00      0.00      0.00         0\n",
            "             Alexandre_Dumas       0.00      0.00      0.00         1\n",
            "             Alphonse_Daudet       0.00      0.00      0.00         1\n",
            "               Anton_Chekhov       1.00      1.00      1.00         1\n",
            "                Aristophanes       0.00      0.00      0.00         1\n",
            "          Arthur_Conan_Doyle       0.00      0.00      0.00         1\n",
            "                      Borges       0.00      0.00      0.00         0\n",
            "             Boris_Pasternak       0.00      0.00      0.00         0\n",
            "          Charles_Baudelaire       0.00      0.00      0.00         0\n",
            "             Charles_Dickens       0.00      0.00      0.00         1\n",
            "            Chingiz_Aitmatov       0.00      0.00      0.00         0\n",
            "             Dante_Alighieri       0.00      0.00      0.00         1\n",
            "             Edgar_Allan_Poe       0.00      0.00      0.00         1\n",
            "            Ernest_Hemingway       0.00      0.00      0.00         0\n",
            "          F_Scott_Fitzgerald       0.00      0.00      0.00         1\n",
            "       Federico_Garcia_Lorca       0.00      0.00      0.00         0\n",
            "                 Franz_Kafka       0.33      1.00      0.50         1\n",
            "          Fyodor_Dostoyevsky       1.00      1.00      1.00         1\n",
            "            Geoffrey_Chaucer       1.00      1.00      1.00         1\n",
            "               George_Orwell       0.00      0.00      0.00         1\n",
            "          Giovanni_Boccaccio       1.00      1.00      1.00         1\n",
            "            Graf_Leo_Tolstoy       0.00      0.00      0.00         1\n",
            "           Guy_De_Maupassant       0.00      0.00      0.00         1\n",
            "               H_P_Lovecraft       0.00      0.00      0.00         1\n",
            "     Hans_Christian_Andersen       1.00      1.00      1.00         1\n",
            "             Haruki_Murakami       0.00      0.00      0.00         1\n",
            "             Herman_Melville       0.00      0.00      0.00         1\n",
            "            Honore_De_Balzac       1.00      1.00      1.00         1\n",
            "                Isaac_Asimov       0.00      0.00      0.00         1\n",
            "               Italo_Calvino       1.00      1.00      1.00         1\n",
            "               Ivan_Turgenev       1.00      1.00      1.00         1\n",
            "               J_R_R_Tolkien       0.08      1.00      0.14         1\n",
            "                 Jack_London       0.00      0.00      0.00         1\n",
            "    Jalaluddin_Muhammad_Rumi       0.00      0.00      0.00         0\n",
            "       James_Fenimore_Cooper       1.00      1.00      1.00         1\n",
            "                 James_Joyce       0.00      0.00      0.00         1\n",
            "                 Jane_Austen       0.00      0.00      0.00         1\n",
            "         Jean_De_La_Fontaine       0.00      0.00      0.00         1\n",
            "                 Jean_Racine       0.00      0.00      0.00         0\n",
            "  Johann_Wolfgang_Von_Goethe       0.00      0.00      0.00         1\n",
            "                 Jules_Verne       0.00      0.00      0.00         1\n",
            "                  Mark_Twain       0.00      0.00      0.00         1\n",
            "                 Maxim_Gorky       0.00      0.00      0.00         1\n",
            "                  Mayne_Reid       0.00      0.00      0.00         1\n",
            "Miguel_De_Cervantes_Saavedra       0.00      0.00      0.00         1\n",
            "           Mikhail_Lermontov       0.00      0.00      0.00         0\n",
            "                     Moliere       0.00      0.00      0.00         1\n",
            "                     O_Henry       0.00      0.00      0.00         1\n",
            "                 Oscar_Wilde       0.00      0.00      0.00         1\n",
            "      Publius_Vergilius_Maro       0.00      0.00      0.00         1\n",
            "          Rainer_Maria_Rilke       0.00      0.00      0.00         0\n",
            "      Robert_Louis_Stevenson       0.00      0.00      0.00         1\n",
            "                   Sophocles       0.00      0.00      0.00         1\n",
            "                    Stendhal       0.00      0.00      0.00         1\n",
            "                   T_S_Eliot       0.00      0.00      0.00         0\n",
            "                 Thomas_Mann       0.00      0.00      0.00         1\n",
            "                 Umberto_Eco       0.00      0.00      0.00         1\n",
            "            Ursula_K_Le_Guin       0.00      0.00      0.00         0\n",
            "                 Victor_Hugo       0.00      0.00      0.00         1\n",
            "              Virginia_Woolf       1.00      1.00      1.00         1\n",
            "            William_Faulkner       0.00      0.00      0.00         1\n",
            "         William_Shakespeare       0.00      0.00      0.00         1\n",
            "\n",
            "                    accuracy                           0.24        51\n",
            "                   macro avg       0.16      0.19      0.17        51\n",
            "                weighted avg       0.20      0.24      0.21        51\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.2353,\n",
              " 'precision_macro': 0.1627,\n",
              " 'recall_macro': 0.1875,\n",
              " 'f1_macro': 0.1663,\n",
              " 'precision_micro': 0.2353,\n",
              " 'recall_micro': 0.2353,\n",
              " 'f1_micro': 0.2353}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем обучить логистическую регрессию"
      ],
      "metadata": {
        "id": "XFOAbHAPMACR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Создаем конвейер для масштабирования данных и обучения модели\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Обучаем модель на тренировочных данных\n",
        "model.fit(X_train_np, ytrain)\n",
        "\n",
        "# Делаем предсказания на тестовых данных\n",
        "y_pred = model.predict(X_test_np)\n",
        "\n",
        "# Оцениваем результаты\n",
        "calculate_multiclass_metrics(ytest.tolist(), y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMp_MaXawhBL",
        "outputId": "3b0baeb4-d801-4048-9629-1f27affdfa5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "                       Aesop       0.00      0.00      0.00         1\n",
            "             Agatha_Christie       0.00      0.00      0.00         1\n",
            "        Ahmet_Hamdi_Tanpinar       0.00      0.00      0.00         0\n",
            "               Aldous_Huxley       0.00      0.00      0.00         0\n",
            "             Alexandre_Dumas       0.00      0.00      0.00         1\n",
            "             Alphonse_Daudet       1.00      1.00      1.00         1\n",
            "               Anton_Chekhov       0.00      0.00      0.00         1\n",
            "                Aristophanes       0.00      0.00      0.00         1\n",
            "          Arthur_Conan_Doyle       0.00      0.00      0.00         1\n",
            "              Bertolt_Brecht       0.00      0.00      0.00         0\n",
            "             Charles_Dickens       0.00      0.00      0.00         1\n",
            "             Dante_Alighieri       0.00      0.00      0.00         1\n",
            "             Edgar_Allan_Poe       0.00      0.00      0.00         1\n",
            "          F_Scott_Fitzgerald       0.33      1.00      0.50         1\n",
            "       Federico_Garcia_Lorca       0.00      0.00      0.00         0\n",
            "                 Franz_Kafka       0.00      0.00      0.00         1\n",
            "          Fyodor_Dostoyevsky       1.00      1.00      1.00         1\n",
            "            Geoffrey_Chaucer       1.00      1.00      1.00         1\n",
            "         George_Bernard_Shaw       0.00      0.00      0.00         0\n",
            "               George_Orwell       0.00      0.00      0.00         1\n",
            "          Giovanni_Boccaccio       0.00      0.00      0.00         1\n",
            "            Graf_Leo_Tolstoy       0.00      0.00      0.00         1\n",
            "           Guy_De_Maupassant       0.00      0.00      0.00         1\n",
            "               H_P_Lovecraft       0.00      0.00      0.00         1\n",
            "     Hans_Christian_Andersen       1.00      1.00      1.00         1\n",
            "             Haruki_Murakami       1.00      1.00      1.00         1\n",
            "             Herman_Melville       0.00      0.00      0.00         1\n",
            "            Honore_De_Balzac       0.50      1.00      0.67         1\n",
            "                Isaac_Asimov       0.00      0.00      0.00         1\n",
            "               Italo_Calvino       0.33      1.00      0.50         1\n",
            "               Ivan_Turgenev       0.50      1.00      0.67         1\n",
            "               J_R_R_Tolkien       0.00      0.00      0.00         1\n",
            "                 Jack_London       0.00      0.00      0.00         1\n",
            "       James_Fenimore_Cooper       0.00      0.00      0.00         1\n",
            "                 James_Joyce       0.00      0.00      0.00         1\n",
            "                 Jane_Austen       0.00      0.00      0.00         1\n",
            "         Jean_De_La_Fontaine       0.00      0.00      0.00         1\n",
            "  Johann_Wolfgang_Von_Goethe       0.00      0.00      0.00         1\n",
            "                  John_Fante       0.00      0.00      0.00         0\n",
            "                 Jules_Verne       0.00      0.00      0.00         1\n",
            "                 Lev_Tolstoy       0.00      0.00      0.00         0\n",
            "                  Mark_Twain       0.00      0.00      0.00         1\n",
            "                 Maxim_Gorky       0.00      0.00      0.00         1\n",
            "                  Mayne_Reid       0.00      0.00      0.00         1\n",
            "Miguel_De_Cervantes_Saavedra       0.00      0.00      0.00         1\n",
            "                     Moliere       0.00      0.00      0.00         1\n",
            "                     O_Henry       0.00      0.00      0.00         1\n",
            "                 Oscar_Wilde       0.00      0.00      0.00         1\n",
            "      Publius_Vergilius_Maro       0.00      0.00      0.00         1\n",
            "      Robert_Louis_Stevenson       0.00      0.00      0.00         1\n",
            "              Salman_Rushdie       0.00      0.00      0.00         0\n",
            "                   Sophocles       0.50      1.00      0.67         1\n",
            "                    Stendhal       0.00      0.00      0.00         1\n",
            "                   T_S_Eliot       0.00      0.00      0.00         0\n",
            "                 Thomas_Mann       0.00      0.00      0.00         1\n",
            "                 Umberto_Eco       0.17      1.00      0.29         1\n",
            "            Ursula_K_Le_Guin       0.00      0.00      0.00         0\n",
            "                 Victor_Hugo       0.50      1.00      0.67         1\n",
            "              Virginia_Woolf       0.33      1.00      0.50         1\n",
            "            William_Faulkner       0.00      0.00      0.00         1\n",
            "         William_Shakespeare       0.00      0.00      0.00         1\n",
            "\n",
            "                    accuracy                           0.25        51\n",
            "                   macro avg       0.13      0.21      0.15        51\n",
            "                weighted avg       0.16      0.25      0.19        51\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.2549,\n",
              " 'precision_macro': 0.1339,\n",
              " 'recall_macro': 0.2131,\n",
              " 'f1_macro': 0.155,\n",
              " 'precision_micro': 0.2549,\n",
              " 'recall_micro': 0.2549,\n",
              " 'f1_micro': 0.2549}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем уменьшить размерности и найдём косинусную близость"
      ],
      "metadata": {
        "id": "RF6uBFihMEHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Преобразуем серии в numpy массивы\n",
        "X_train_np = np.array(X_train.tolist())\n",
        "X_test_np = np.array(X_test.tolist())\n",
        "\n",
        "# Применяем PCA для уменьшения размерности\n",
        "pca = PCA(n_components=35)  # Оставляем достаточное количество компонент для объяснения 95% дисперсии\n",
        "X_train_pca = pca.fit_transform(X_train_np)\n",
        "X_test_pca = pca.transform(X_test_np)\n",
        "\n",
        "# Рассчитываем косинусную близость на уменьшенной размерности\n",
        "similarity_matrix = cosine_similarity(X_test_pca, X_train_pca)\n",
        "\n",
        "# Инициализируем список для предсказанных меток\n",
        "y_pred = []\n",
        "\n",
        "# Для каждого вектора в X_test находим наиболее близкий вектор в X_train\n",
        "for i in range(len(X_test_pca)):\n",
        "    # Находим индекс наиболее близкого вектора в X_train\n",
        "    closest_index = np.argmax(similarity_matrix[i])\n",
        "\n",
        "    # Сохраняем соответствующую метку класса из ytrain\n",
        "    predicted_label = ytrain.iloc[closest_index]\n",
        "\n",
        "    # Добавляем метку в список предсказаний\n",
        "    y_pred.append(predicted_label)\n",
        "\n",
        "# Вызываем вашу функцию для оценки результатов\n",
        "calculate_multiclass_metrics(ytest.tolist(), y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uwqCUaCxnvv",
        "outputId": "d07e9e4d-e925-4b8d-b6f4-f33aaa01005e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "                       Aesop       0.00      0.00      0.00         1\n",
            "             Agatha_Christie       0.00      0.00      0.00         1\n",
            "        Ahmet_Hamdi_Tanpinar       0.00      0.00      0.00         0\n",
            "               Aldous_Huxley       0.00      0.00      0.00         0\n",
            "             Alexandre_Dumas       1.00      1.00      1.00         1\n",
            "             Alphonse_Daudet       1.00      1.00      1.00         1\n",
            "               Anton_Chekhov       0.00      0.00      0.00         1\n",
            "                Aristophanes       0.00      0.00      0.00         1\n",
            "          Arthur_Conan_Doyle       0.00      0.00      0.00         1\n",
            "              Arthur_Rimbaud       0.00      0.00      0.00         0\n",
            "                Beaumarchais       0.00      0.00      0.00         0\n",
            "             Boris_Pasternak       0.00      0.00      0.00         0\n",
            "          Charles_Baudelaire       0.00      0.00      0.00         0\n",
            "             Charles_Dickens       0.00      0.00      0.00         1\n",
            "             Dante_Alighieri       0.00      0.00      0.00         1\n",
            "             Edgar_Allan_Poe       0.00      0.00      0.00         1\n",
            "          F_Scott_Fitzgerald       0.00      0.00      0.00         1\n",
            "                 Franz_Kafka       1.00      1.00      1.00         1\n",
            "          Fyodor_Dostoyevsky       1.00      1.00      1.00         1\n",
            "            Geoffrey_Chaucer       1.00      1.00      1.00         1\n",
            "               George_Orwell       0.00      0.00      0.00         1\n",
            "          Giovanni_Boccaccio       1.00      1.00      1.00         1\n",
            "            Graf_Leo_Tolstoy       0.00      0.00      0.00         1\n",
            "           Guy_De_Maupassant       0.00      0.00      0.00         1\n",
            "               H_P_Lovecraft       0.00      0.00      0.00         1\n",
            "     Hans_Christian_Andersen       1.00      1.00      1.00         1\n",
            "             Haruki_Murakami       1.00      1.00      1.00         1\n",
            "             Herman_Melville       0.00      0.00      0.00         1\n",
            "            Honore_De_Balzac       1.00      1.00      1.00         1\n",
            "                Isaac_Asimov       0.00      0.00      0.00         1\n",
            "               Italo_Calvino       1.00      1.00      1.00         1\n",
            "               Ivan_Turgenev       1.00      1.00      1.00         1\n",
            "               J_R_R_Tolkien       0.00      0.00      0.00         1\n",
            "                 Jack_London       0.00      0.00      0.00         1\n",
            "       James_Fenimore_Cooper       0.00      0.00      0.00         1\n",
            "                 James_Joyce       0.00      0.00      0.00         1\n",
            "                 Jane_Austen       0.00      0.00      0.00         1\n",
            "         Jean_De_La_Fontaine       0.00      0.00      0.00         1\n",
            "            Jean_Paul_Sartre       0.00      0.00      0.00         0\n",
            "  Johann_Wolfgang_Von_Goethe       0.00      0.00      0.00         1\n",
            "                  John_Keats       0.00      0.00      0.00         0\n",
            "                 Jules_Verne       0.00      0.00      0.00         1\n",
            "                 Lev_Tolstoy       0.00      0.00      0.00         0\n",
            "                  Lord_Byron       0.00      0.00      0.00         0\n",
            "                  Mark_Twain       0.00      0.00      0.00         1\n",
            "                 Maxim_Gorky       0.00      0.00      0.00         1\n",
            "                  Mayne_Reid       0.00      0.00      0.00         1\n",
            "Miguel_De_Cervantes_Saavedra       1.00      1.00      1.00         1\n",
            "                     Moliere       0.00      0.00      0.00         1\n",
            "                     O_Henry       0.00      0.00      0.00         1\n",
            "                 Oscar_Wilde       0.00      0.00      0.00         1\n",
            "      Publius_Vergilius_Maro       0.00      0.00      0.00         1\n",
            "          Rainer_Maria_Rilke       0.00      0.00      0.00         0\n",
            "      Robert_Louis_Stevenson       0.00      0.00      0.00         1\n",
            "              Salman_Rushdie       0.00      0.00      0.00         0\n",
            "                   Sophocles       1.00      1.00      1.00         1\n",
            "                Stefan_Zweig       0.00      0.00      0.00         0\n",
            "                    Stendhal       0.00      0.00      0.00         1\n",
            "                   T_S_Eliot       0.00      0.00      0.00         0\n",
            "                 Thomas_Mann       0.00      0.00      0.00         1\n",
            "              Thomas_Pynchon       0.00      0.00      0.00         0\n",
            "                 Umberto_Eco       1.00      1.00      1.00         1\n",
            "                 Victor_Hugo       1.00      1.00      1.00         1\n",
            "              Virginia_Woolf       1.00      1.00      1.00         1\n",
            "            William_Faulkner       0.00      0.00      0.00         1\n",
            "         William_Shakespeare       0.00      0.00      0.00         1\n",
            "\n",
            "                    accuracy                           0.31        51\n",
            "                   macro avg       0.24      0.24      0.24        51\n",
            "                weighted avg       0.31      0.31      0.31        51\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.3137,\n",
              " 'precision_macro': 0.2424,\n",
              " 'recall_macro': 0.2424,\n",
              " 'f1_macro': 0.2424,\n",
              " 'precision_micro': 0.3137,\n",
              " 'recall_micro': 0.3137,\n",
              " 'f1_micro': 0.3137}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем обучить логистическую регрессию с настроенным гиперпараметром C"
      ],
      "metadata": {
        "id": "QxrNbOBAMRpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Создаем конвейер для масштабирования данных и обучения модели\n",
        "model = LogisticRegression(max_iter=1000, C=1.015)\n",
        "\n",
        "# Обучаем модель на тренировочных данных\n",
        "model.fit(X_train_pca, ytrain)\n",
        "\n",
        "# Делаем предсказания на тестовых данных\n",
        "y_pred = model.predict(X_test_pca)\n",
        "\n",
        "# Оцениваем результаты\n",
        "calculate_multiclass_metrics(ytest.tolist(), y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xij-z724ArC",
        "outputId": "b94314f9-d74a-4347-f76d-b5cfe6b72d60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "                       Aesop       0.00      0.00      0.00         1\n",
            "             Agatha_Christie       0.00      0.00      0.00         1\n",
            "        Ahmet_Hamdi_Tanpinar       0.00      0.00      0.00         0\n",
            "               Aldous_Huxley       0.00      0.00      0.00         0\n",
            "             Alexandre_Dumas       0.00      0.00      0.00         1\n",
            "             Alphonse_Daudet       1.00      1.00      1.00         1\n",
            "               Anton_Chekhov       0.00      0.00      0.00         1\n",
            "                Aristophanes       0.00      0.00      0.00         1\n",
            "          Arthur_Conan_Doyle       0.00      0.00      0.00         1\n",
            "              Arthur_Rimbaud       0.00      0.00      0.00         0\n",
            "                Beaumarchais       0.00      0.00      0.00         0\n",
            "             Boris_Pasternak       0.00      0.00      0.00         0\n",
            "          Charles_Baudelaire       0.00      0.00      0.00         0\n",
            "             Charles_Dickens       0.00      0.00      0.00         1\n",
            "             Dante_Alighieri       0.00      0.00      0.00         1\n",
            "             Edgar_Allan_Poe       0.00      0.00      0.00         1\n",
            "          F_Scott_Fitzgerald       0.00      0.00      0.00         1\n",
            "                 Franz_Kafka       1.00      1.00      1.00         1\n",
            "          Fyodor_Dostoyevsky       0.50      1.00      0.67         1\n",
            "      Gabriel_Garcia_Marquez       0.00      0.00      0.00         0\n",
            "            Geoffrey_Chaucer       1.00      1.00      1.00         1\n",
            "               George_Orwell       0.00      0.00      0.00         1\n",
            "          Giovanni_Boccaccio       0.00      0.00      0.00         1\n",
            "            Graf_Leo_Tolstoy       0.00      0.00      0.00         1\n",
            "           Guy_De_Maupassant       0.00      0.00      0.00         1\n",
            "               H_P_Lovecraft       0.00      0.00      0.00         1\n",
            "     Hans_Christian_Andersen       0.00      0.00      0.00         1\n",
            "             Haruki_Murakami       1.00      1.00      1.00         1\n",
            "             Herman_Melville       0.00      0.00      0.00         1\n",
            "            Honore_De_Balzac       0.00      0.00      0.00         1\n",
            "                Isaac_Asimov       0.00      0.00      0.00         1\n",
            "               Italo_Calvino       0.50      1.00      0.67         1\n",
            "               Ivan_Turgenev       0.00      0.00      0.00         1\n",
            "               J_R_R_Tolkien       0.00      0.00      0.00         1\n",
            "                 Jack_London       0.00      0.00      0.00         1\n",
            "       James_Fenimore_Cooper       0.00      0.00      0.00         1\n",
            "                 James_Joyce       0.00      0.00      0.00         1\n",
            "                 Jane_Austen       0.00      0.00      0.00         1\n",
            "         Jean_De_La_Fontaine       0.00      0.00      0.00         1\n",
            "            Jean_Paul_Sartre       0.00      0.00      0.00         0\n",
            "  Johann_Wolfgang_Von_Goethe       0.00      0.00      0.00         1\n",
            "                 Jules_Verne       0.00      0.00      0.00         1\n",
            "                  Lord_Byron       0.00      0.00      0.00         0\n",
            "          Mario_Vargas_Llosa       0.00      0.00      0.00         0\n",
            "                  Mark_Twain       0.00      0.00      0.00         1\n",
            "                 Maxim_Gorky       0.00      0.00      0.00         1\n",
            "                  Mayne_Reid       0.00      0.00      0.00         1\n",
            "Miguel_De_Cervantes_Saavedra       0.00      0.00      0.00         1\n",
            "                     Moliere       0.00      0.00      0.00         1\n",
            "              Naguib_Mahfouz       0.00      0.00      0.00         0\n",
            "                 Neil_Gaiman       0.00      0.00      0.00         0\n",
            "                     O_Henry       0.00      0.00      0.00         1\n",
            "                 Oscar_Wilde       0.00      0.00      0.00         1\n",
            "      Publius_Vergilius_Maro       0.00      0.00      0.00         1\n",
            "          Rainer_Maria_Rilke       0.00      0.00      0.00         0\n",
            "      Robert_Louis_Stevenson       0.00      0.00      0.00         1\n",
            "                   Sophocles       0.50      1.00      0.67         1\n",
            "                    Stendhal       0.00      0.00      0.00         1\n",
            "                   T_S_Eliot       0.00      0.00      0.00         0\n",
            "                 Thomas_Mann       0.00      0.00      0.00         1\n",
            "              Thomas_Pynchon       0.00      0.00      0.00         0\n",
            "                 Umberto_Eco       0.50      1.00      0.67         1\n",
            "                 Victor_Hugo       1.00      1.00      1.00         1\n",
            "              Virginia_Woolf       0.00      0.00      0.00         1\n",
            "            William_Faulkner       0.00      0.00      0.00         1\n",
            "         William_Shakespeare       0.00      0.00      0.00         1\n",
            "\n",
            "                    accuracy                           0.18        51\n",
            "                   macro avg       0.11      0.14      0.12        51\n",
            "                weighted avg       0.14      0.18      0.15        51\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.1765,\n",
              " 'precision_macro': 0.1061,\n",
              " 'recall_macro': 0.1364,\n",
              " 'f1_macro': 0.1162,\n",
              " 'precision_micro': 0.1765,\n",
              " 'recall_micro': 0.1765,\n",
              " 'f1_micro': 0.1765}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим LinearSVC с StandardScaler"
      ],
      "metadata": {
        "id": "fY9RqiAZMW70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model = make_pipeline(\n",
        "                      StandardScaler(),\n",
        "                      LinearSVC(max_iter=1000, C=0.99))\n",
        "\n",
        "model.fit(X_train_np, ytrain)\n",
        "y_pred = model.predict(X_test_np)\n",
        "\n",
        "calculate_multiclass_metrics(ytest, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEMB3wsH6sD5",
        "outputId": "af49ad4b-d3bb-4bed-a049-927a32062bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "                       Aesop       0.00      0.00      0.00         1\n",
            "             Agatha_Christie       0.00      0.00      0.00         1\n",
            "             Alexandre_Dumas       0.00      0.00      0.00         1\n",
            "             Alphonse_Daudet       0.00      0.00      0.00         1\n",
            "               Anton_Chekhov       1.00      1.00      1.00         1\n",
            "                Aristophanes       0.00      0.00      0.00         1\n",
            "          Arthur_Conan_Doyle       0.00      0.00      0.00         1\n",
            "                      Borges       0.00      0.00      0.00         0\n",
            "             Boris_Pasternak       0.00      0.00      0.00         0\n",
            "             Charles_Dickens       0.00      0.00      0.00         1\n",
            "            Chingiz_Aitmatov       0.00      0.00      0.00         0\n",
            "             Dante_Alighieri       0.00      0.00      0.00         1\n",
            "             Edgar_Allan_Poe       0.00      0.00      0.00         1\n",
            "            Ernest_Hemingway       0.00      0.00      0.00         0\n",
            "          F_Scott_Fitzgerald       1.00      1.00      1.00         1\n",
            "       Federico_Garcia_Lorca       0.00      0.00      0.00         0\n",
            "                 Franz_Kafka       1.00      1.00      1.00         1\n",
            "          Fyodor_Dostoyevsky       1.00      1.00      1.00         1\n",
            "            Geoffrey_Chaucer       1.00      1.00      1.00         1\n",
            "               George_Orwell       0.00      0.00      0.00         1\n",
            "          Giovanni_Boccaccio       0.00      0.00      0.00         1\n",
            "            Graf_Leo_Tolstoy       0.00      0.00      0.00         1\n",
            "           Guy_De_Maupassant       0.00      0.00      0.00         1\n",
            "               H_P_Lovecraft       0.00      0.00      0.00         1\n",
            "     Hans_Christian_Andersen       1.00      1.00      1.00         1\n",
            "             Haruki_Murakami       0.00      0.00      0.00         1\n",
            "             Herman_Melville       0.00      0.00      0.00         1\n",
            "            Honore_De_Balzac       1.00      1.00      1.00         1\n",
            "                Isaac_Asimov       0.00      0.00      0.00         1\n",
            "               Italo_Calvino       1.00      1.00      1.00         1\n",
            "               Ivan_Turgenev       1.00      1.00      1.00         1\n",
            "               J_R_R_Tolkien       0.08      1.00      0.14         1\n",
            "                 Jack_London       0.00      0.00      0.00         1\n",
            "    Jalaluddin_Muhammad_Rumi       0.00      0.00      0.00         0\n",
            "       James_Fenimore_Cooper       1.00      1.00      1.00         1\n",
            "                 James_Joyce       0.00      0.00      0.00         1\n",
            "                 Jane_Austen       0.00      0.00      0.00         1\n",
            "         Jean_De_La_Fontaine       0.00      0.00      0.00         1\n",
            "  Johann_Wolfgang_Von_Goethe       0.00      0.00      0.00         1\n",
            "                 Jules_Verne       0.00      0.00      0.00         1\n",
            "                  Mark_Twain       0.00      0.00      0.00         1\n",
            "                 Maxim_Gorky       0.00      0.00      0.00         1\n",
            "                  Mayne_Reid       0.00      0.00      0.00         1\n",
            "Miguel_De_Cervantes_Saavedra       0.00      0.00      0.00         1\n",
            "                     Moliere       0.00      0.00      0.00         1\n",
            "                     O_Henry       0.00      0.00      0.00         1\n",
            "                 Oscar_Wilde       0.00      0.00      0.00         1\n",
            "      Publius_Vergilius_Maro       0.00      0.00      0.00         1\n",
            "          Rainer_Maria_Rilke       0.00      0.00      0.00         0\n",
            "      Robert_Louis_Stevenson       0.00      0.00      0.00         1\n",
            "                   Sophocles       0.00      0.00      0.00         1\n",
            "                    Stendhal       0.00      0.00      0.00         1\n",
            "                   T_S_Eliot       0.00      0.00      0.00         0\n",
            "                 Thomas_Mann       0.00      0.00      0.00         1\n",
            "                 Umberto_Eco       0.00      0.00      0.00         1\n",
            "                 Victor_Hugo       0.00      0.00      0.00         1\n",
            "              Virginia_Woolf       1.00      1.00      1.00         1\n",
            "            William_Faulkner       0.00      0.00      0.00         1\n",
            "         William_Shakespeare       0.00      0.00      0.00         1\n",
            "\n",
            "                    accuracy                           0.24        51\n",
            "                   macro avg       0.19      0.20      0.19        51\n",
            "                weighted avg       0.22      0.24      0.22        51\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.2353,\n",
              " 'precision_macro': 0.1877,\n",
              " 'recall_macro': 0.2034,\n",
              " 'f1_macro': 0.1889,\n",
              " 'precision_micro': 0.2353,\n",
              " 'recall_micro': 0.2353,\n",
              " 'f1_micro': 0.2353}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим RidgeClassifier с StandardScaler"
      ],
      "metadata": {
        "id": "rejBqNniMhlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import RidgeClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model = make_pipeline(StandardScaler(),\n",
        "                      RidgeClassifier(max_iter=1000))\n",
        "\n",
        "model.fit(X_train_np, ytrain)\n",
        "y_pred = model.predict(X_test_np)\n",
        "\n",
        "calculate_multiclass_metrics(ytest, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItbmOcmd7_fO",
        "outputId": "202d3403-0287-4e52-9817-dbb3aee52625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "                       Aesop       0.00      0.00      0.00         1\n",
            "             Agatha_Christie       0.00      0.00      0.00         1\n",
            "             Alexandre_Dumas       0.00      0.00      0.00         1\n",
            "             Alphonse_Daudet       0.00      0.00      0.00         1\n",
            "               Anton_Chekhov       1.00      1.00      1.00         1\n",
            "                Aristophanes       0.00      0.00      0.00         1\n",
            "          Arthur_Conan_Doyle       0.00      0.00      0.00         1\n",
            "              Bertolt_Brecht       0.00      0.00      0.00         0\n",
            "                      Borges       0.00      0.00      0.00         0\n",
            "             Boris_Pasternak       0.00      0.00      0.00         0\n",
            "          Charles_Baudelaire       0.00      0.00      0.00         0\n",
            "             Charles_Dickens       0.00      0.00      0.00         1\n",
            "            Chingiz_Aitmatov       0.00      0.00      0.00         0\n",
            "             Dante_Alighieri       0.00      0.00      0.00         1\n",
            "             Edgar_Allan_Poe       0.00      0.00      0.00         1\n",
            "            Ernest_Hemingway       0.00      0.00      0.00         0\n",
            "          F_Scott_Fitzgerald       1.00      1.00      1.00         1\n",
            "       Federico_Garcia_Lorca       0.00      0.00      0.00         0\n",
            "                 Franz_Kafka       0.25      1.00      0.40         1\n",
            "          Fyodor_Dostoyevsky       1.00      1.00      1.00         1\n",
            "            Geoffrey_Chaucer       1.00      1.00      1.00         1\n",
            "               George_Orwell       0.00      0.00      0.00         1\n",
            "          Giovanni_Boccaccio       0.00      0.00      0.00         1\n",
            "            Graf_Leo_Tolstoy       0.00      0.00      0.00         1\n",
            "           Guy_De_Maupassant       0.00      0.00      0.00         1\n",
            "               H_P_Lovecraft       0.00      0.00      0.00         1\n",
            "     Hans_Christian_Andersen       1.00      1.00      1.00         1\n",
            "             Haruki_Murakami       0.00      0.00      0.00         1\n",
            "             Herman_Melville       0.00      0.00      0.00         1\n",
            "            Honore_De_Balzac       1.00      1.00      1.00         1\n",
            "                Isaac_Asimov       0.00      0.00      0.00         1\n",
            "               Italo_Calvino       1.00      1.00      1.00         1\n",
            "               Ivan_Turgenev       1.00      1.00      1.00         1\n",
            "               J_R_R_Tolkien       0.00      0.00      0.00         1\n",
            "                 Jack_London       0.00      0.00      0.00         1\n",
            "    Jalaluddin_Muhammad_Rumi       0.00      0.00      0.00         0\n",
            "       James_Fenimore_Cooper       1.00      1.00      1.00         1\n",
            "                 James_Joyce       0.00      0.00      0.00         1\n",
            "                 Jane_Austen       1.00      1.00      1.00         1\n",
            "         Jean_De_La_Fontaine       0.00      0.00      0.00         1\n",
            "  Johann_Wolfgang_Von_Goethe       0.00      0.00      0.00         1\n",
            "                 Jules_Verne       0.00      0.00      0.00         1\n",
            "                  Mark_Twain       0.00      0.00      0.00         1\n",
            "                 Maxim_Gorky       0.00      0.00      0.00         1\n",
            "                  Mayne_Reid       0.00      0.00      0.00         1\n",
            "Miguel_De_Cervantes_Saavedra       0.00      0.00      0.00         1\n",
            "           Mikhail_Lermontov       0.00      0.00      0.00         0\n",
            "                     Moliere       0.00      0.00      0.00         1\n",
            "                     O_Henry       0.00      0.00      0.00         1\n",
            "                 Oscar_Wilde       0.00      0.00      0.00         1\n",
            "      Publius_Vergilius_Maro       0.00      0.00      0.00         1\n",
            "          Rainer_Maria_Rilke       0.00      0.00      0.00         0\n",
            "      Robert_Louis_Stevenson       0.00      0.00      0.00         1\n",
            "                   Sophocles       0.00      0.00      0.00         1\n",
            "                    Stendhal       0.00      0.00      0.00         1\n",
            "                   T_S_Eliot       0.00      0.00      0.00         0\n",
            "                 Thomas_Mann       0.00      0.00      0.00         1\n",
            "                 Umberto_Eco       0.00      0.00      0.00         1\n",
            "                 Victor_Hugo       0.00      0.00      0.00         1\n",
            "              Virginia_Woolf       1.00      1.00      1.00         1\n",
            "            William_Faulkner       0.00      0.00      0.00         1\n",
            "         William_Shakespeare       0.00      0.00      0.00         1\n",
            "\n",
            "                    accuracy                           0.24        51\n",
            "                   macro avg       0.18      0.19      0.18        51\n",
            "                weighted avg       0.22      0.24      0.22        51\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.2353,\n",
              " 'precision_macro': 0.1815,\n",
              " 'recall_macro': 0.1935,\n",
              " 'f1_macro': 0.1839,\n",
              " 'precision_micro': 0.2353,\n",
              " 'recall_micro': 0.2353,\n",
              " 'f1_micro': 0.2353}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим MultinomialNB с MinMaxScaler"
      ],
      "metadata": {
        "id": "17v4cwu0MlIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model = make_pipeline(MinMaxScaler(),\n",
        "                      # StandardScaler(),\n",
        "                      MultinomialNB())\n",
        "\n",
        "model.fit(X_train_pca, ytrain)\n",
        "\n",
        "y_pred = model.predict(X_test_pca)\n",
        "\n",
        "calculate_multiclass_metrics(ytest, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4MIkyIPt9_Q6",
        "outputId": "6dd9779e-2d6c-4e80-df2f-9af0533e148e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "                       Aesop       0.00      0.00      0.00         1\n",
            "             Agatha_Christie       0.00      0.00      0.00         1\n",
            "        Ahmet_Hamdi_Tanpinar       0.00      0.00      0.00         0\n",
            "                Albert_Camus       0.00      0.00      0.00         0\n",
            "               Aldous_Huxley       0.00      0.00      0.00         0\n",
            "             Alexandre_Dumas       1.00      1.00      1.00         1\n",
            "             Alphonse_Daudet       1.00      1.00      1.00         1\n",
            "               Anton_Chekhov       0.00      0.00      0.00         1\n",
            "                Aristophanes       0.00      0.00      0.00         1\n",
            "          Arthur_Conan_Doyle       0.00      0.00      0.00         1\n",
            "              Bertolt_Brecht       0.00      0.00      0.00         0\n",
            "             Boris_Pasternak       0.00      0.00      0.00         0\n",
            "             Charles_Dickens       0.00      0.00      0.00         1\n",
            "             Dante_Alighieri       0.00      0.00      0.00         1\n",
            "             Edgar_Allan_Poe       0.00      0.00      0.00         1\n",
            "          F_Scott_Fitzgerald       1.00      1.00      1.00         1\n",
            "           Francois_Rabelais       0.00      0.00      0.00         0\n",
            "                 Franz_Kafka       1.00      1.00      1.00         1\n",
            "          Fyodor_Dostoyevsky       1.00      1.00      1.00         1\n",
            "            Geoffrey_Chaucer       1.00      1.00      1.00         1\n",
            "               George_Orwell       0.00      0.00      0.00         1\n",
            "          Giovanni_Boccaccio       0.00      0.00      0.00         1\n",
            "            Graf_Leo_Tolstoy       0.00      0.00      0.00         1\n",
            "           Guy_De_Maupassant       0.00      0.00      0.00         1\n",
            "               H_P_Lovecraft       0.00      0.00      0.00         1\n",
            "     Hans_Christian_Andersen       1.00      1.00      1.00         1\n",
            "             Haruki_Murakami       1.00      1.00      1.00         1\n",
            "             Herman_Melville       0.00      0.00      0.00         1\n",
            "            Honore_De_Balzac       1.00      1.00      1.00         1\n",
            "                Isaac_Asimov       0.00      0.00      0.00         1\n",
            "               Italo_Calvino       0.25      1.00      0.40         1\n",
            "               Ivan_Turgenev       1.00      1.00      1.00         1\n",
            "               J_R_R_Tolkien       1.00      1.00      1.00         1\n",
            "                 Jack_London       0.00      0.00      0.00         1\n",
            "       James_Fenimore_Cooper       0.00      0.00      0.00         1\n",
            "                 James_Joyce       0.00      0.00      0.00         1\n",
            "                 Jane_Austen       0.00      0.00      0.00         1\n",
            "         Jean_De_La_Fontaine       0.00      0.00      0.00         1\n",
            "            Jean_Paul_Sartre       0.00      0.00      0.00         0\n",
            "                 Jean_Racine       0.00      0.00      0.00         0\n",
            "  Johann_Wolfgang_Von_Goethe       0.00      0.00      0.00         1\n",
            "                  John_Keats       0.00      0.00      0.00         0\n",
            "                 Jules_Verne       0.00      0.00      0.00         1\n",
            "                  Lord_Byron       0.00      0.00      0.00         0\n",
            "      Louis_Ferdinand_Celine       0.00      0.00      0.00         0\n",
            "          Mario_Vargas_Llosa       0.00      0.00      0.00         0\n",
            "                  Mark_Twain       0.00      0.00      0.00         1\n",
            "                 Maxim_Gorky       1.00      1.00      1.00         1\n",
            "                  Mayne_Reid       0.00      0.00      0.00         1\n",
            "Miguel_De_Cervantes_Saavedra       1.00      1.00      1.00         1\n",
            "               Milan_Kundera       0.00      0.00      0.00         0\n",
            "                     Moliere       0.00      0.00      0.00         1\n",
            "              Naguib_Mahfouz       0.00      0.00      0.00         0\n",
            "                     O_Henry       0.00      0.00      0.00         1\n",
            "                 Oscar_Wilde       0.00      0.00      0.00         1\n",
            "      Publius_Vergilius_Maro       0.00      0.00      0.00         1\n",
            "          Rainer_Maria_Rilke       0.00      0.00      0.00         0\n",
            "      Robert_Louis_Stevenson       0.00      0.00      0.00         1\n",
            "                   Sophocles       0.50      1.00      0.67         1\n",
            "                    Stendhal       0.00      0.00      0.00         1\n",
            "                   T_S_Eliot       0.00      0.00      0.00         0\n",
            "                 Thomas_Mann       0.00      0.00      0.00         1\n",
            "                 Umberto_Eco       1.00      1.00      1.00         1\n",
            "                 Victor_Hugo       1.00      1.00      1.00         1\n",
            "              Virginia_Woolf       1.00      1.00      1.00         1\n",
            "            William_Faulkner       0.00      0.00      0.00         1\n",
            "         William_Shakespeare       0.00      0.00      0.00         1\n",
            "\n",
            "                    accuracy                           0.35        51\n",
            "                   macro avg       0.25      0.27      0.25        51\n",
            "                weighted avg       0.33      0.35      0.33        51\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.3529,\n",
              " 'precision_macro': 0.25,\n",
              " 'recall_macro': 0.2687,\n",
              " 'f1_macro': 0.2547,\n",
              " 'precision_micro': 0.3529,\n",
              " 'recall_micro': 0.3529,\n",
              " 'f1_micro': 0.3529}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим MultinomialNB с TfidfVectorizer"
      ],
      "metadata": {
        "id": "3bvXqD2hMuDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "import joblib\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "model = MultinomialNB()\n",
        "\n",
        "pipeline = make_pipeline(vectorizer, model)\n",
        "\n",
        "pipeline.fit(df_train['text'], ytrain)\n",
        "y_pred = pipeline.predict(df_test['text'])\n",
        "\n",
        "calculate_multiclass_metrics(ytest.tolist(), y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Imad4OIeqlow",
        "outputId": "e5777454-0672-429c-80aa-18c206a36b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "                       Aesop       0.00      0.00      0.00         1\n",
            "             Agatha_Christie       0.00      0.00      0.00         1\n",
            "             Alexandre_Dumas       0.50      1.00      0.67         1\n",
            "             Alphonse_Daudet       1.00      1.00      1.00         1\n",
            "               Anton_Chekhov       0.00      0.00      0.00         1\n",
            "                Aristophanes       1.00      1.00      1.00         1\n",
            "          Arthur_Conan_Doyle       0.00      0.00      0.00         1\n",
            "                      Borges       0.00      0.00      0.00         0\n",
            "             Charles_Dickens       0.00      0.00      0.00         1\n",
            "             Dante_Alighieri       0.00      0.00      0.00         1\n",
            "             Edgar_Allan_Poe       0.50      1.00      0.67         1\n",
            "          F_Scott_Fitzgerald       0.00      0.00      0.00         1\n",
            "                 Franz_Kafka       1.00      1.00      1.00         1\n",
            "          Fyodor_Dostoyevsky       1.00      1.00      1.00         1\n",
            "            Geoffrey_Chaucer       1.00      1.00      1.00         1\n",
            "               George_Orwell       0.00      0.00      0.00         1\n",
            "          Giovanni_Boccaccio       0.50      1.00      0.67         1\n",
            "            Graf_Leo_Tolstoy       0.00      0.00      0.00         1\n",
            "           Guy_De_Maupassant       0.00      0.00      0.00         1\n",
            "               H_P_Lovecraft       0.00      0.00      0.00         1\n",
            "     Hans_Christian_Andersen       0.50      1.00      0.67         1\n",
            "             Haruki_Murakami       1.00      1.00      1.00         1\n",
            "             Herman_Melville       0.00      0.00      0.00         1\n",
            "            Honore_De_Balzac       0.20      1.00      0.33         1\n",
            "                Isaac_Asimov       0.25      1.00      0.40         1\n",
            "               Italo_Calvino       1.00      1.00      1.00         1\n",
            "               Ivan_Turgenev       1.00      1.00      1.00         1\n",
            "               J_R_R_Tolkien       1.00      1.00      1.00         1\n",
            "                 Jack_London       0.00      0.00      0.00         1\n",
            "       James_Fenimore_Cooper       0.00      0.00      0.00         1\n",
            "                 James_Joyce       0.00      0.00      0.00         1\n",
            "                 Jane_Austen       1.00      1.00      1.00         1\n",
            "         Jean_De_La_Fontaine       0.00      0.00      0.00         1\n",
            "  Johann_Wolfgang_Von_Goethe       1.00      1.00      1.00         1\n",
            "              John_Steinbeck       0.00      0.00      0.00         0\n",
            "                 Jules_Verne       0.00      0.00      0.00         1\n",
            "                  Mark_Twain       0.00      0.00      0.00         1\n",
            "                 Maxim_Gorky       1.00      1.00      1.00         1\n",
            "                  Mayne_Reid       0.00      0.00      0.00         1\n",
            "Miguel_De_Cervantes_Saavedra       1.00      1.00      1.00         1\n",
            "               Milan_Kundera       0.00      0.00      0.00         0\n",
            "                     Moliere       0.00      0.00      0.00         1\n",
            "                     O_Henry       1.00      1.00      1.00         1\n",
            "                 Oscar_Wilde       0.00      0.00      0.00         1\n",
            "      Publius_Vergilius_Maro       1.00      1.00      1.00         1\n",
            "      Robert_Louis_Stevenson       0.00      0.00      0.00         1\n",
            "                   Sophocles       1.00      1.00      1.00         1\n",
            "                Stefan_Zweig       0.00      0.00      0.00         0\n",
            "                    Stendhal       0.00      0.00      0.00         1\n",
            "                   T_S_Eliot       0.00      0.00      0.00         0\n",
            "                 Thomas_Mann       0.00      0.00      0.00         1\n",
            "                 Umberto_Eco       1.00      1.00      1.00         1\n",
            "                 Victor_Hugo       1.00      1.00      1.00         1\n",
            "              Virginia_Woolf       1.00      1.00      1.00         1\n",
            "            William_Faulkner       0.00      0.00      0.00         1\n",
            "         William_Shakespeare       0.00      0.00      0.00         1\n",
            "\n",
            "                    accuracy                           0.49        51\n",
            "                   macro avg       0.38      0.45      0.40        51\n",
            "                weighted avg       0.42      0.49      0.44        51\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.4902,\n",
              " 'precision_macro': 0.383,\n",
              " 'recall_macro': 0.4464,\n",
              " 'f1_macro': 0.4,\n",
              " 'precision_micro': 0.4902,\n",
              " 'recall_micro': 0.4902,\n",
              " 'f1_micro': 0.4902}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем обучить GaussianNB"
      ],
      "metadata": {
        "id": "Y6HsLpJcUSeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model = make_pipeline(#StandardScaler(),\n",
        "                      GaussianNB())\n",
        "\n",
        "model.fit(X_train_pca, ytrain)\n",
        "\n",
        "y_pred = model.predict(X_test_pca)\n",
        "\n",
        "calculate_multiclass_metrics(ytest, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSEIa_3g-xYx",
        "outputId": "7ee7915e-f348-4883-bdf7-546a4d94cfe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "                       Aesop       0.00      0.00      0.00         1\n",
            "             Agatha_Christie       0.00      0.00      0.00         1\n",
            "             Alexandre_Dumas       0.00      0.00      0.00         1\n",
            "             Alphonse_Daudet       1.00      1.00      1.00         1\n",
            "    Antoine_De_Saint_Exupery       0.00      0.00      0.00         0\n",
            "               Anton_Chekhov       0.50      1.00      0.67         1\n",
            "                Aristophanes       0.00      0.00      0.00         1\n",
            "          Arthur_Conan_Doyle       0.00      0.00      0.00         1\n",
            "              Arthur_Rimbaud       0.00      0.00      0.00         0\n",
            "             Boris_Pasternak       0.00      0.00      0.00         0\n",
            "             Charles_Dickens       0.00      0.00      0.00         1\n",
            "             Dante_Alighieri       0.00      0.00      0.00         1\n",
            "             Edgar_Allan_Poe       0.00      0.00      0.00         1\n",
            "        Erich_Maria_Remarque       0.00      0.00      0.00         0\n",
            "          F_Scott_Fitzgerald       0.00      0.00      0.00         1\n",
            "       Federico_Garcia_Lorca       0.00      0.00      0.00         0\n",
            "                 Franz_Kafka       1.00      1.00      1.00         1\n",
            "          Fyodor_Dostoyevsky       1.00      1.00      1.00         1\n",
            "      Gabriel_Garcia_Marquez       0.00      0.00      0.00         0\n",
            "            Geoffrey_Chaucer       1.00      1.00      1.00         1\n",
            "               George_Orwell       0.00      0.00      0.00         1\n",
            "          Giovanni_Boccaccio       0.00      0.00      0.00         1\n",
            "            Graf_Leo_Tolstoy       0.00      0.00      0.00         1\n",
            "           Guy_De_Maupassant       0.00      0.00      0.00         1\n",
            "               H_P_Lovecraft       0.00      0.00      0.00         1\n",
            "     Hans_Christian_Andersen       0.50      1.00      0.67         1\n",
            "             Haruki_Murakami       1.00      1.00      1.00         1\n",
            "             Herman_Melville       0.00      0.00      0.00         1\n",
            "            Honore_De_Balzac       1.00      1.00      1.00         1\n",
            "                Isaac_Asimov       0.00      0.00      0.00         1\n",
            "               Italo_Calvino       1.00      1.00      1.00         1\n",
            "               Ivan_Turgenev       0.25      1.00      0.40         1\n",
            "               J_R_R_Tolkien       0.00      0.00      0.00         1\n",
            "                 Jack_London       0.00      0.00      0.00         1\n",
            "       James_Fenimore_Cooper       0.00      0.00      0.00         1\n",
            "                 James_Joyce       0.00      0.00      0.00         1\n",
            "                 Jane_Austen       0.00      0.00      0.00         1\n",
            "         Jean_De_La_Fontaine       0.00      0.00      0.00         1\n",
            "            Jean_Paul_Sartre       0.00      0.00      0.00         0\n",
            "  Johann_Wolfgang_Von_Goethe       0.00      0.00      0.00         1\n",
            "                  John_Keats       0.00      0.00      0.00         0\n",
            "                 Jules_Verne       0.00      0.00      0.00         1\n",
            "              Julio_Cortazar       0.00      0.00      0.00         0\n",
            "                 Lev_Tolstoy       0.00      0.00      0.00         0\n",
            "                  Lord_Byron       0.00      0.00      0.00         0\n",
            "                  Mark_Twain       0.00      0.00      0.00         1\n",
            "                 Maxim_Gorky       1.00      1.00      1.00         1\n",
            "                  Mayne_Reid       0.00      0.00      0.00         1\n",
            "Miguel_De_Cervantes_Saavedra       1.00      1.00      1.00         1\n",
            "               Milan_Kundera       0.00      0.00      0.00         0\n",
            "                     Moliere       0.00      0.00      0.00         1\n",
            "                     O_Henry       0.00      0.00      0.00         1\n",
            "                 Oscar_Wilde       0.00      0.00      0.00         1\n",
            "      Publius_Vergilius_Maro       0.25      1.00      0.40         1\n",
            "      Robert_Louis_Stevenson       0.00      0.00      0.00         1\n",
            "              Salman_Rushdie       0.00      0.00      0.00         0\n",
            "                   Sophocles       0.00      0.00      0.00         1\n",
            "                    Stendhal       0.00      0.00      0.00         1\n",
            "                 Thomas_Mann       0.00      0.00      0.00         1\n",
            "                 Umberto_Eco       1.00      1.00      1.00         1\n",
            "            Ursula_K_Le_Guin       0.00      0.00      0.00         0\n",
            "                 Victor_Hugo       1.00      1.00      1.00         1\n",
            "              Virginia_Woolf       1.00      1.00      1.00         1\n",
            "            William_Faulkner       0.00      0.00      0.00         1\n",
            "         William_Shakespeare       0.00      0.00      0.00         1\n",
            "\n",
            "                    accuracy                           0.31        51\n",
            "                   macro avg       0.21      0.25      0.22        51\n",
            "                weighted avg       0.26      0.31      0.28        51\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.3137,\n",
              " 'precision_macro': 0.2077,\n",
              " 'recall_macro': 0.2462,\n",
              " 'f1_macro': 0.2174,\n",
              " 'precision_micro': 0.3137,\n",
              " 'recall_micro': 0.3137,\n",
              " 'f1_micro': 0.3137}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим LogisticRegression с MinMaxScaler на данных после PCA"
      ],
      "metadata": {
        "id": "JylmuVIDUYyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Создаем конвейер для масштабирования данных и обучения модели\n",
        "model = make_pipeline(MinMaxScaler(),\n",
        "                      LogisticRegression(max_iter=1000))\n",
        "# Обучаем модель на тренировочных данных\n",
        "model.fit(X_train_pca, ytrain)\n",
        "\n",
        "# Делаем предсказания на тестовых данных\n",
        "y_pred = model.predict(X_test_pca)\n",
        "\n",
        "# Оцениваем результаты\n",
        "calculate_multiclass_metrics(ytest.tolist(), y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i10La_O1__F2",
        "outputId": "7726c2e0-0e18-4b35-ff61-60672ab8347f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "                       Aesop       0.00      0.00      0.00         1\n",
            "             Agatha_Christie       0.00      0.00      0.00         1\n",
            "        Ahmet_Hamdi_Tanpinar       0.00      0.00      0.00         0\n",
            "             Alexandre_Dumas       1.00      1.00      1.00         1\n",
            "             Alphonse_Daudet       0.00      0.00      0.00         1\n",
            "               Anton_Chekhov       0.00      0.00      0.00         1\n",
            "                Aristophanes       0.00      0.00      0.00         1\n",
            "          Arthur_Conan_Doyle       0.00      0.00      0.00         1\n",
            "             Boris_Pasternak       0.00      0.00      0.00         0\n",
            "             Charles_Dickens       0.00      0.00      0.00         1\n",
            "             Dante_Alighieri       0.00      0.00      0.00         1\n",
            "             Edgar_Allan_Poe       0.00      0.00      0.00         1\n",
            "          F_Scott_Fitzgerald       0.33      1.00      0.50         1\n",
            "       Federico_Garcia_Lorca       0.00      0.00      0.00         0\n",
            "           Francois_Rabelais       0.00      0.00      0.00         0\n",
            "                 Franz_Kafka       1.00      1.00      1.00         1\n",
            "          Fyodor_Dostoyevsky       1.00      1.00      1.00         1\n",
            "            Geoffrey_Chaucer       1.00      1.00      1.00         1\n",
            "               George_Orwell       0.00      0.00      0.00         1\n",
            "          Giovanni_Boccaccio       0.00      0.00      0.00         1\n",
            "            Graf_Leo_Tolstoy       0.00      0.00      0.00         1\n",
            "           Guy_De_Maupassant       0.00      0.00      0.00         1\n",
            "               H_P_Lovecraft       0.00      0.00      0.00         1\n",
            "     Hans_Christian_Andersen       1.00      1.00      1.00         1\n",
            "             Haruki_Murakami       1.00      1.00      1.00         1\n",
            "             Herman_Melville       0.00      0.00      0.00         1\n",
            "            Honore_De_Balzac       1.00      1.00      1.00         1\n",
            "                Isaac_Asimov       1.00      1.00      1.00         1\n",
            "               Italo_Calvino       1.00      1.00      1.00         1\n",
            "               Ivan_Turgenev       0.00      0.00      0.00         1\n",
            "               J_R_R_Tolkien       1.00      1.00      1.00         1\n",
            "                 Jack_London       0.00      0.00      0.00         1\n",
            "       James_Fenimore_Cooper       0.00      0.00      0.00         1\n",
            "                 James_Joyce       0.00      0.00      0.00         1\n",
            "                 Jane_Austen       0.00      0.00      0.00         1\n",
            "         Jean_De_La_Fontaine       0.00      0.00      0.00         1\n",
            "  Johann_Wolfgang_Von_Goethe       0.00      0.00      0.00         1\n",
            "                 Jules_Verne       0.00      0.00      0.00         1\n",
            "      Louis_Ferdinand_Celine       0.00      0.00      0.00         0\n",
            "                  Mark_Twain       0.00      0.00      0.00         1\n",
            "                 Maxim_Gorky       1.00      1.00      1.00         1\n",
            "                  Mayne_Reid       0.00      0.00      0.00         1\n",
            "Miguel_De_Cervantes_Saavedra       1.00      1.00      1.00         1\n",
            "               Milan_Kundera       0.00      0.00      0.00         0\n",
            "                     Moliere       0.00      0.00      0.00         1\n",
            "           Nikos_Kazantzakis       0.00      0.00      0.00         0\n",
            "                     O_Henry       0.00      0.00      0.00         1\n",
            "                 Oscar_Wilde       0.00      0.00      0.00         1\n",
            "      Publius_Vergilius_Maro       0.00      0.00      0.00         1\n",
            "          Rainer_Maria_Rilke       0.00      0.00      0.00         0\n",
            "      Robert_Louis_Stevenson       0.00      0.00      0.00         1\n",
            "                   Sophocles       0.00      0.00      0.00         1\n",
            "                    Stendhal       0.00      0.00      0.00         1\n",
            "                 Thomas_Mann       0.00      0.00      0.00         1\n",
            "              Thomas_Pynchon       0.00      0.00      0.00         0\n",
            "                 Umberto_Eco       1.00      1.00      1.00         1\n",
            "                 Victor_Hugo       1.00      1.00      1.00         1\n",
            "              Virginia_Woolf       1.00      1.00      1.00         1\n",
            "            William_Faulkner       0.00      0.00      0.00         1\n",
            "         William_Shakespeare       0.00      0.00      0.00         1\n",
            "\n",
            "                    accuracy                           0.31        51\n",
            "                   macro avg       0.26      0.27      0.26        51\n",
            "                weighted avg       0.30      0.31      0.30        51\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.3137,\n",
              " 'precision_macro': 0.2556,\n",
              " 'recall_macro': 0.2667,\n",
              " 'f1_macro': 0.2583,\n",
              " 'precision_micro': 0.3137,\n",
              " 'recall_micro': 0.3137,\n",
              " 'f1_micro': 0.3137}"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавим векторизацию текстов, при помощи мешка слов"
      ],
      "metadata": {
        "id": "4K_v8528uAmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cnt_vec = CountVectorizer(lowercase=False, token_pattern=r'(?u)\\b\\w+\\b')\n",
        "X = cnt_vec.fit_transform(df_train.text)"
      ],
      "metadata": {
        "id": "Sb-CUzFnt5li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = dict(sorted(cnt_vec.vocabulary_.items(), key=lambda item: item[1]))\n",
        "pd.DataFrame(X.toarray(), columns=res.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "gSN4KIuxuHsm",
        "outputId": "e97e445f-eb4e-40e9-f97c-b2984ef07aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      A  AAA  AAAgnes  AACR  AAs  AB  ABAFT  ABANDON  ABANDONED  ABBA  ...  \\\n",
              "0    68    0        0     0    0   0      0        0          0     0  ...   \n",
              "1     8    0        0     0    0   0      0        0          0     0  ...   \n",
              "2   105    0        0     0    0   0      0        0          0     0  ...   \n",
              "3   413    0        0     0    0   1      0        0          0     0  ...   \n",
              "4    50    0        0     0    0   0      0        0          0     0  ...   \n",
              "..  ...  ...      ...   ...  ...  ..    ...      ...        ...   ...  ...   \n",
              "95  103    0        0     0    0   0      0        0          0     0  ...   \n",
              "96   19    0        0     0    0   0      0        0          0     0  ...   \n",
              "97  198    0        0     0    0   0      0        0          0     0  ...   \n",
              "98  145    0        0     0    0   0      0        0          0     0  ...   \n",
              "99  130    0        0     0    0   0      0        0          0     0  ...   \n",
              "\n",
              "    Şerefinize  Şişhane  Şânizâde  Şîr  Şükrü  şalvars  şarkı  şerike  \\\n",
              "0            0        0         0    0      0        0      0       0   \n",
              "1            0        0         0    0      0        0      0       0   \n",
              "2            0        0         0    0      0        0      0       0   \n",
              "3            0        0         0    0      0        0      0       0   \n",
              "4            0        0         0    0      0        0      0       0   \n",
              "..         ...      ...       ...  ...    ...      ...    ...     ...   \n",
              "95           0        0         0    0      0        0      0       0   \n",
              "96           0        0         0    0      0        0      0       0   \n",
              "97           1        1         1    1      1        1      1       1   \n",
              "98           0        0         0    0      0        0      0       0   \n",
              "99           0        0         0    0      0        0      0       0   \n",
              "\n",
              "    сovering  сzerov  \n",
              "0          0       0  \n",
              "1          0       0  \n",
              "2          0       0  \n",
              "3          0       0  \n",
              "4          0       0  \n",
              "..       ...     ...  \n",
              "95         0       0  \n",
              "96         0       0  \n",
              "97         0       0  \n",
              "98         0       0  \n",
              "99         0       0  \n",
              "\n",
              "[100 rows x 189203 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1c64a123-1585-4b56-92a8-9289159d21a0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A</th>\n",
              "      <th>AAA</th>\n",
              "      <th>AAAgnes</th>\n",
              "      <th>AACR</th>\n",
              "      <th>AAs</th>\n",
              "      <th>AB</th>\n",
              "      <th>ABAFT</th>\n",
              "      <th>ABANDON</th>\n",
              "      <th>ABANDONED</th>\n",
              "      <th>ABBA</th>\n",
              "      <th>...</th>\n",
              "      <th>Şerefinize</th>\n",
              "      <th>Şişhane</th>\n",
              "      <th>Şânizâde</th>\n",
              "      <th>Şîr</th>\n",
              "      <th>Şükrü</th>\n",
              "      <th>şalvars</th>\n",
              "      <th>şarkı</th>\n",
              "      <th>şerike</th>\n",
              "      <th>сovering</th>\n",
              "      <th>сzerov</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>68</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>105</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>413</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>50</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>103</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>198</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>145</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>130</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 189203 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1c64a123-1585-4b56-92a8-9289159d21a0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1c64a123-1585-4b56-92a8-9289159d21a0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1c64a123-1585-4b56-92a8-9289159d21a0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e5e5225d-553a-41c5-b14b-23bb5c7fcad8\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e5e5225d-553a-41c5-b14b-23bb5c7fcad8')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e5e5225d-553a-41c5-b14b-23bb5c7fcad8 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим LogisticRegression на BoW"
      ],
      "metadata": {
        "id": "x_KCpEQuUmHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "\n",
        "vec = CountVectorizer(ngram_range=(1, 1))\n",
        "\n",
        "bow = vec.fit_transform(df_train.text)\n",
        "bow_test = vec.transform(df_test.text)\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "bow = scaler.fit_transform(bow)\n",
        "bow_test = scaler.transform(bow_test)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000, random_state=42)\n",
        "clf.fit(bow, ytrain)\n",
        "y_pred = clf.predict(bow_test)\n",
        "\n",
        "calculate_multiclass_metrics(ytest.tolist(), y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zBI1lM9wj5W",
        "outputId": "7a7b12b7-4ec6-480e-bf90-f21a75927fd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "                       Aesop       0.00      0.00      0.00         1\n",
            "             Agatha_Christie       0.00      0.00      0.00         1\n",
            "             Alexandre_Dumas       1.00      1.00      1.00         1\n",
            "             Alphonse_Daudet       1.00      1.00      1.00         1\n",
            "    Antoine_De_Saint_Exupery       0.00      0.00      0.00         0\n",
            "               Anton_Chekhov       0.00      0.00      0.00         1\n",
            "                Aristophanes       0.00      0.00      0.00         1\n",
            "          Arthur_Conan_Doyle       0.00      0.00      0.00         1\n",
            "             Boris_Pasternak       0.00      0.00      0.00         0\n",
            "             Charles_Dickens       0.00      0.00      0.00         1\n",
            "             Dante_Alighieri       0.00      0.00      0.00         1\n",
            "             Edgar_Allan_Poe       0.00      0.00      0.00         1\n",
            "          F_Scott_Fitzgerald       0.00      0.00      0.00         1\n",
            "       Federico_Garcia_Lorca       0.00      0.00      0.00         0\n",
            "                 Franz_Kafka       1.00      1.00      1.00         1\n",
            "          Fyodor_Dostoyevsky       1.00      1.00      1.00         1\n",
            "            Geoffrey_Chaucer       1.00      1.00      1.00         1\n",
            "               George_Orwell       0.00      0.00      0.00         1\n",
            "          Giovanni_Boccaccio       1.00      1.00      1.00         1\n",
            "            Graf_Leo_Tolstoy       0.00      0.00      0.00         1\n",
            "           Guy_De_Maupassant       0.00      0.00      0.00         1\n",
            "               H_P_Lovecraft       0.00      0.00      0.00         1\n",
            "     Hans_Christian_Andersen       1.00      1.00      1.00         1\n",
            "             Haruki_Murakami       1.00      1.00      1.00         1\n",
            "             Herman_Melville       0.00      0.00      0.00         1\n",
            "            Honore_De_Balzac       1.00      1.00      1.00         1\n",
            "                Isaac_Asimov       0.00      0.00      0.00         1\n",
            "               Italo_Calvino       1.00      1.00      1.00         1\n",
            "               Ivan_Turgenev       1.00      1.00      1.00         1\n",
            "               J_R_R_Tolkien       1.00      1.00      1.00         1\n",
            "                 Jack_London       0.00      0.00      0.00         1\n",
            "       James_Fenimore_Cooper       1.00      1.00      1.00         1\n",
            "                 James_Joyce       0.00      0.00      0.00         1\n",
            "                 Jane_Austen       0.00      0.00      0.00         1\n",
            "         Jean_De_La_Fontaine       0.00      0.00      0.00         1\n",
            "  Johann_Wolfgang_Von_Goethe       0.00      0.00      0.00         1\n",
            "                 Jules_Verne       0.00      0.00      0.00         1\n",
            "              Julio_Cortazar       0.00      0.00      0.00         0\n",
            "                  Mark_Twain       0.00      0.00      0.00         1\n",
            "                 Maxim_Gorky       1.00      1.00      1.00         1\n",
            "                  Mayne_Reid       0.00      0.00      0.00         1\n",
            "Miguel_De_Cervantes_Saavedra       1.00      1.00      1.00         1\n",
            "                     Moliere       0.00      0.00      0.00         1\n",
            "                     O_Henry       0.00      0.00      0.00         1\n",
            "                 Oscar_Wilde       0.00      0.00      0.00         1\n",
            "                Pablo_Neruda       0.00      0.00      0.00         0\n",
            "      Publius_Vergilius_Maro       0.00      0.00      0.00         1\n",
            "      Robert_Louis_Stevenson       0.00      0.00      0.00         1\n",
            "                   Sophocles       1.00      1.00      1.00         1\n",
            "                    Stendhal       1.00      1.00      1.00         1\n",
            "                   T_S_Eliot       0.00      0.00      0.00         0\n",
            "                 Thomas_Mann       0.00      0.00      0.00         1\n",
            "                 Umberto_Eco       1.00      1.00      1.00         1\n",
            "                 Victor_Hugo       1.00      1.00      1.00         1\n",
            "              Virginia_Woolf       1.00      1.00      1.00         1\n",
            "            William_Faulkner       0.00      0.00      0.00         1\n",
            "         William_Shakespeare       1.00      1.00      1.00         1\n",
            "\n",
            "                    accuracy                           0.41        51\n",
            "                   macro avg       0.37      0.37      0.37        51\n",
            "                weighted avg       0.41      0.41      0.41        51\n",
            "\n",
            "CPU times: user 2min 53s, sys: 28.4 s, total: 3min 21s\n",
            "Wall time: 3min 15s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.4118,\n",
              " 'precision_macro': 0.3684,\n",
              " 'recall_macro': 0.3684,\n",
              " 'f1_macro': 0.3684,\n",
              " 'precision_micro': 0.4118,\n",
              " 'recall_micro': 0.4118,\n",
              " 'f1_micro': 0.4118}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим датафреймы с BoW"
      ],
      "metadata": {
        "id": "ITXo2JBbUvId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bow_train_df = pd.DataFrame(bow.toarray(), columns=vec.get_feature_names_out())\n",
        "bow_test_df = pd.DataFrame(bow_test.toarray(), columns=vec.get_feature_names_out())"
      ],
      "metadata": {
        "id": "98M-iZQHyu7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим LogisticRegression на векторах полученных со статистик и обработанных PCA объединённых с BoW"
      ],
      "metadata": {
        "id": "USOxKpy_UzaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "X_train_pca_df = pd.DataFrame(X_train_pca)\n",
        "X_test_pca_df = pd.DataFrame(X_test_pca)\n",
        "bow_train_df = pd.concat([bow_train_df, X_train_pca_df], axis=1)\n",
        "bow_test_df = pd.concat([bow_test_df, X_test_pca_df], axis=1)\n",
        "bow_train_df.columns = bow_train_df.columns.astype(str)\n",
        "bow_test_df.columns = bow_test_df.columns.astype(str)\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "bow_train_df = scaler.fit_transform(bow_train_df)\n",
        "bow_test_df = scaler.transform(bow_test_df)\n",
        "\n",
        "clf = LogisticRegression(max_iter=333, random_state=42)\n",
        "clf.fit(bow_train_df, ytrain)\n",
        "bow_vec_pred = clf.predict(bow_test_df)\n",
        "\n",
        "calculate_multiclass_metrics(ytest.tolist(), bow_vec_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vCjyKXzZyhg_",
        "outputId": "d1aa8891-e04f-48a5-c066-1103a84b7e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "                       Aesop       0.00      0.00      0.00         1\n",
            "             Agatha_Christie       0.00      0.00      0.00         1\n",
            "             Alexandre_Dumas       1.00      1.00      1.00         1\n",
            "             Alphonse_Daudet       1.00      1.00      1.00         1\n",
            "    Antoine_De_Saint_Exupery       0.00      0.00      0.00         0\n",
            "               Anton_Chekhov       0.00      0.00      0.00         1\n",
            "                Aristophanes       0.00      0.00      0.00         1\n",
            "          Arthur_Conan_Doyle       0.00      0.00      0.00         1\n",
            "             Boris_Pasternak       0.00      0.00      0.00         0\n",
            "             Charles_Dickens       0.00      0.00      0.00         1\n",
            "             Dante_Alighieri       0.00      0.00      0.00         1\n",
            "             Edgar_Allan_Poe       0.00      0.00      0.00         1\n",
            "          F_Scott_Fitzgerald       0.00      0.00      0.00         1\n",
            "       Federico_Garcia_Lorca       0.00      0.00      0.00         0\n",
            "                 Franz_Kafka       1.00      1.00      1.00         1\n",
            "          Fyodor_Dostoyevsky       1.00      1.00      1.00         1\n",
            "            Geoffrey_Chaucer       1.00      1.00      1.00         1\n",
            "               George_Orwell       0.00      0.00      0.00         1\n",
            "          Giovanni_Boccaccio       1.00      1.00      1.00         1\n",
            "            Graf_Leo_Tolstoy       0.00      0.00      0.00         1\n",
            "           Guy_De_Maupassant       0.00      0.00      0.00         1\n",
            "               H_P_Lovecraft       0.00      0.00      0.00         1\n",
            "     Hans_Christian_Andersen       1.00      1.00      1.00         1\n",
            "             Haruki_Murakami       1.00      1.00      1.00         1\n",
            "             Herman_Melville       0.00      0.00      0.00         1\n",
            "            Honore_De_Balzac       1.00      1.00      1.00         1\n",
            "                Isaac_Asimov       0.00      0.00      0.00         1\n",
            "               Italo_Calvino       1.00      1.00      1.00         1\n",
            "               Ivan_Turgenev       1.00      1.00      1.00         1\n",
            "               J_R_R_Tolkien       1.00      1.00      1.00         1\n",
            "                 Jack_London       0.00      0.00      0.00         1\n",
            "       James_Fenimore_Cooper       1.00      1.00      1.00         1\n",
            "                 James_Joyce       0.00      0.00      0.00         1\n",
            "                 Jane_Austen       0.00      0.00      0.00         1\n",
            "         Jean_De_La_Fontaine       0.00      0.00      0.00         1\n",
            "  Johann_Wolfgang_Von_Goethe       0.00      0.00      0.00         1\n",
            "                 Jules_Verne       0.00      0.00      0.00         1\n",
            "              Julio_Cortazar       0.00      0.00      0.00         0\n",
            "                  Mark_Twain       0.00      0.00      0.00         1\n",
            "                 Maxim_Gorky       1.00      1.00      1.00         1\n",
            "                  Mayne_Reid       0.00      0.00      0.00         1\n",
            "Miguel_De_Cervantes_Saavedra       1.00      1.00      1.00         1\n",
            "                     Moliere       0.00      0.00      0.00         1\n",
            "                     O_Henry       0.00      0.00      0.00         1\n",
            "                 Oscar_Wilde       0.00      0.00      0.00         1\n",
            "                Pablo_Neruda       0.00      0.00      0.00         0\n",
            "      Publius_Vergilius_Maro       0.00      0.00      0.00         1\n",
            "      Robert_Louis_Stevenson       0.00      0.00      0.00         1\n",
            "                   Sophocles       1.00      1.00      1.00         1\n",
            "                    Stendhal       1.00      1.00      1.00         1\n",
            "                   T_S_Eliot       0.00      0.00      0.00         0\n",
            "                 Thomas_Mann       0.00      0.00      0.00         1\n",
            "                 Umberto_Eco       1.00      1.00      1.00         1\n",
            "                 Victor_Hugo       1.00      1.00      1.00         1\n",
            "              Virginia_Woolf       1.00      1.00      1.00         1\n",
            "            William_Faulkner       0.00      0.00      0.00         1\n",
            "         William_Shakespeare       1.00      1.00      1.00         1\n",
            "\n",
            "                    accuracy                           0.41        51\n",
            "                   macro avg       0.37      0.37      0.37        51\n",
            "                weighted avg       0.41      0.41      0.41        51\n",
            "\n",
            "CPU times: user 3min 34s, sys: 32.1 s, total: 4min 6s\n",
            "Wall time: 3min 50s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.4118,\n",
              " 'precision_macro': 0.3684,\n",
              " 'recall_macro': 0.3684,\n",
              " 'f1_macro': 0.3684,\n",
              " 'precision_micro': 0.4118,\n",
              " 'recall_micro': 0.4118,\n",
              " 'f1_micro': 0.4118}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выполним Tfidf векторизацию текстов"
      ],
      "metadata": {
        "id": "PuXoH6oRuKDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vec = TfidfVectorizer(lowercase=False, token_pattern=r'(?u)\\b\\w+\\b')\n",
        "X2 = tfidf_vec.fit_transform(df_train.text)"
      ],
      "metadata": {
        "id": "RtvWXlLguLPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = dict(sorted(cnt_vec.vocabulary_.items(), key=lambda item: item[1]))\n",
        "pd.DataFrame(X2.toarray(), columns=res.keys())"
      ],
      "metadata": {
        "id": "2PZIWrtOuOBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оставим для эксперементов только тексты"
      ],
      "metadata": {
        "id": "5W4lKZKyVSA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = df_train.text\n",
        "test = df_test.text"
      ],
      "metadata": {
        "id": "goOpODCjP0ch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df_train, df_test"
      ],
      "metadata": {
        "id": "ECQle_d1P_s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем обучить LogisticRegression на Tfidf с биграммами, обучить не вышло, из-за нехватки памяти"
      ],
      "metadata": {
        "id": "Q7Cle17GVZVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "vec = TfidfVectorizer(ngram_range=(2, 2))\n",
        "vec_train = vec.fit_transform(train)\n",
        "vec_test = vec.transform(test)\n",
        "\n",
        "scaler = MaxAbsScaler()\n",
        "vec_train = scaler.fit_transform(vec_train)\n",
        "vec_test = scaler.transform(vec_test)\n",
        "\n",
        "clf = LogisticRegression(max_iter=333, random_state=42)\n",
        "clf.fit(vec_train, ytrain)\n",
        "pred_tfidf = clf.predict(vec_test)\n",
        "\n",
        "calculate_multiclass_metrics(ytest.tolist(), pred_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "PCg7E-oUQDYQ",
        "outputId": "518b7171-5669-4676-df94-79e1a9997269"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'LogisticRegression' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f3450f767ebb>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mvec_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m333\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mpred_tfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LogisticRegression' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Лучшая модель TfidfVectorizer(ngram_range=(1, 2)) с OneVsRestClassifier, MaxAbsScaler и LR"
      ],
      "metadata": {
        "id": "qYqxSRroVvWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "import joblib\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "base_model = LogisticRegression(solver='liblinear')\n",
        "ovr = OneVsRestClassifier(base_model)\n",
        "scaler = MaxAbsScaler()\n",
        "pipeline = make_pipeline(vectorizer, scaler, ovr)\n",
        "\n",
        "pipeline.fit(train, ytrain)\n",
        "lr_pred = pipeline.predict(test)\n",
        "\n",
        "joblib.dump(pipeline, 'pipeline.pkl')\n",
        "\n",
        "lr_pred_prob = pipeline.predict_proba(test)\n",
        "lr_probs = lr_pred_prob[:, 1]\n",
        "\n",
        "calculate_multiclass_metrics(ytest.tolist(), lr_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TZ6kqvEw8wz",
        "outputId": "892d4253-e15e-4213-f060-48a0d1a9fd7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "                       Aesop       0.50      1.00      0.67         1\n",
            "             Agatha_Christie       1.00      1.00      1.00         1\n",
            "             Alexandre_Dumas       1.00      1.00      1.00         1\n",
            "             Alphonse_Daudet       1.00      1.00      1.00         1\n",
            "               Anton_Chekhov       1.00      1.00      1.00         1\n",
            "                Aristophanes       1.00      1.00      1.00         1\n",
            "          Arthur_Conan_Doyle       1.00      1.00      1.00         1\n",
            "             Charles_Dickens       0.00      0.00      0.00         1\n",
            "             Dante_Alighieri       0.00      0.00      0.00         1\n",
            "             Edgar_Allan_Poe       1.00      1.00      1.00         1\n",
            "            Ernest_Hemingway       0.00      0.00      0.00         0\n",
            "          F_Scott_Fitzgerald       1.00      1.00      1.00         1\n",
            "                 Franz_Kafka       1.00      1.00      1.00         1\n",
            "          Fyodor_Dostoyevsky       1.00      1.00      1.00         1\n",
            "            Geoffrey_Chaucer       1.00      1.00      1.00         1\n",
            "               George_Orwell       0.00      0.00      0.00         1\n",
            "          Giovanni_Boccaccio       0.50      1.00      0.67         1\n",
            "            Graf_Leo_Tolstoy       0.00      0.00      0.00         1\n",
            "           Guy_De_Maupassant       0.00      0.00      0.00         1\n",
            "               H_P_Lovecraft       1.00      1.00      1.00         1\n",
            "     Hans_Christian_Andersen       0.50      1.00      0.67         1\n",
            "             Haruki_Murakami       1.00      1.00      1.00         1\n",
            "             Herman_Melville       1.00      1.00      1.00         1\n",
            "            Honore_De_Balzac       1.00      1.00      1.00         1\n",
            "                Isaac_Asimov       1.00      1.00      1.00         1\n",
            "               Italo_Calvino       1.00      1.00      1.00         1\n",
            "               Ivan_Turgenev       1.00      1.00      1.00         1\n",
            "               J_R_R_Tolkien       0.33      1.00      0.50         1\n",
            "                 Jack_London       1.00      1.00      1.00         1\n",
            "       James_Fenimore_Cooper       1.00      1.00      1.00         1\n",
            "                 James_Joyce       0.00      0.00      0.00         1\n",
            "                 Jane_Austen       1.00      1.00      1.00         1\n",
            "         Jean_De_La_Fontaine       0.00      0.00      0.00         1\n",
            "  Johann_Wolfgang_Von_Goethe       0.00      0.00      0.00         1\n",
            "                 Jules_Verne       0.50      1.00      0.67         1\n",
            "              Julio_Cortazar       0.00      0.00      0.00         0\n",
            "                  Lord_Byron       0.00      0.00      0.00         0\n",
            "                  Mark_Twain       1.00      1.00      1.00         1\n",
            "                 Maxim_Gorky       1.00      1.00      1.00         1\n",
            "                  Mayne_Reid       0.00      0.00      0.00         1\n",
            "Miguel_De_Cervantes_Saavedra       1.00      1.00      1.00         1\n",
            "                     Moliere       1.00      1.00      1.00         1\n",
            "                     O_Henry       1.00      1.00      1.00         1\n",
            "                 Oscar_Wilde       0.00      0.00      0.00         1\n",
            "      Publius_Vergilius_Maro       0.00      0.00      0.00         1\n",
            "      Robert_Louis_Stevenson       0.00      0.00      0.00         1\n",
            "                   Sophocles       1.00      1.00      1.00         1\n",
            "                    Stendhal       1.00      1.00      1.00         1\n",
            "                   T_S_Eliot       0.00      0.00      0.00         0\n",
            "                 Thomas_Mann       0.00      0.00      0.00         1\n",
            "                 Umberto_Eco       0.50      1.00      0.67         1\n",
            "                 Victor_Hugo       1.00      1.00      1.00         1\n",
            "              Virginia_Woolf       1.00      1.00      1.00         1\n",
            "            William_Faulkner       0.00      0.00      0.00         1\n",
            "         William_Shakespeare       1.00      1.00      1.00         1\n",
            "\n",
            "                    accuracy                           0.73        51\n",
            "                   macro avg       0.62      0.67      0.63        51\n",
            "                weighted avg       0.66      0.73      0.68        51\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.7255,\n",
              " 'precision_macro': 0.6152,\n",
              " 'recall_macro': 0.6727,\n",
              " 'f1_macro': 0.6333,\n",
              " 'precision_micro': 0.7255,\n",
              " 'recall_micro': 0.7255,\n",
              " 'f1_micro': 0.7255}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем заменить LR в пайплайне прошлой модели на MultinomialNB"
      ],
      "metadata": {
        "id": "kS5x8C-uWDy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "base_model = MultinomialNB()\n",
        "ovr = OneVsRestClassifier(base_model)\n",
        "scaler = MaxAbsScaler()\n",
        "pipeline = make_pipeline(vectorizer, ovr)\n",
        "\n",
        "pipeline.fit(df_train['text'], ytrain)\n",
        "nb_pred = pipeline.predict(df_test['text'])\n",
        "\n",
        "nb_pred_prob = pipeline.predict_proba(df_test['text'])\n",
        "\n",
        "calculate_multiclass_metrics(ytest.tolist(), nb_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECd05MVF0ThQ",
        "outputId": "9e6f6a28-7d3f-4163-9c77-558c09546c4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "                       Aesop       0.50      1.00      0.67         1\n",
            "             Agatha_Christie       0.00      0.00      0.00         1\n",
            "             Alexandre_Dumas       1.00      1.00      1.00         1\n",
            "             Alphonse_Daudet       1.00      1.00      1.00         1\n",
            "               Anton_Chekhov       1.00      1.00      1.00         1\n",
            "                Aristophanes       1.00      1.00      1.00         1\n",
            "          Arthur_Conan_Doyle       0.00      0.00      0.00         1\n",
            "                      Borges       0.00      0.00      0.00         0\n",
            "             Charles_Dickens       0.00      0.00      0.00         1\n",
            "             Dante_Alighieri       0.00      0.00      0.00         1\n",
            "             Edgar_Allan_Poe       0.00      0.00      0.00         1\n",
            "          F_Scott_Fitzgerald       0.00      0.00      0.00         1\n",
            "                 Franz_Kafka       1.00      1.00      1.00         1\n",
            "          Fyodor_Dostoyevsky       1.00      1.00      1.00         1\n",
            "            Geoffrey_Chaucer       1.00      1.00      1.00         1\n",
            "               George_Orwell       0.00      0.00      0.00         1\n",
            "          Giovanni_Boccaccio       0.50      1.00      0.67         1\n",
            "            Graf_Leo_Tolstoy       0.00      0.00      0.00         1\n",
            "           Guy_De_Maupassant       0.00      0.00      0.00         1\n",
            "               H_P_Lovecraft       0.00      0.00      0.00         1\n",
            "     Hans_Christian_Andersen       0.50      1.00      0.67         1\n",
            "             Haruki_Murakami       1.00      1.00      1.00         1\n",
            "             Herman_Melville       0.00      0.00      0.00         1\n",
            "            Honore_De_Balzac       0.25      1.00      0.40         1\n",
            "                Isaac_Asimov       0.25      1.00      0.40         1\n",
            "               Italo_Calvino       1.00      1.00      1.00         1\n",
            "               Ivan_Turgenev       1.00      1.00      1.00         1\n",
            "               J_R_R_Tolkien       1.00      1.00      1.00         1\n",
            "                 Jack_London       0.00      0.00      0.00         1\n",
            "       James_Fenimore_Cooper       0.00      0.00      0.00         1\n",
            "                 James_Joyce       0.00      0.00      0.00         1\n",
            "                 Jane_Austen       1.00      1.00      1.00         1\n",
            "         Jean_De_La_Fontaine       0.00      0.00      0.00         1\n",
            "  Johann_Wolfgang_Von_Goethe       0.00      0.00      0.00         1\n",
            "                 Jules_Verne       0.00      0.00      0.00         1\n",
            "              Kazuo_Ishiguro       0.00      0.00      0.00         0\n",
            "                  Lord_Byron       0.00      0.00      0.00         0\n",
            "                  Mark_Twain       0.00      0.00      0.00         1\n",
            "                 Maxim_Gorky       1.00      1.00      1.00         1\n",
            "                  Mayne_Reid       0.00      0.00      0.00         1\n",
            "Miguel_De_Cervantes_Saavedra       1.00      1.00      1.00         1\n",
            "               Milan_Kundera       0.00      0.00      0.00         0\n",
            "                     Moliere       0.00      0.00      0.00         1\n",
            "                     O_Henry       1.00      1.00      1.00         1\n",
            "                 Oscar_Wilde       0.00      0.00      0.00         1\n",
            "      Publius_Vergilius_Maro       1.00      1.00      1.00         1\n",
            "      Robert_Louis_Stevenson       0.00      0.00      0.00         1\n",
            "                   Sophocles       1.00      1.00      1.00         1\n",
            "                    Stendhal       0.00      0.00      0.00         1\n",
            "                   T_S_Eliot       0.00      0.00      0.00         0\n",
            "                 Thomas_Mann       0.00      0.00      0.00         1\n",
            "                 Umberto_Eco       1.00      1.00      1.00         1\n",
            "                 Victor_Hugo       1.00      1.00      1.00         1\n",
            "              Virginia_Woolf       1.00      1.00      1.00         1\n",
            "            William_Faulkner       0.00      0.00      0.00         1\n",
            "         William_Shakespeare       0.00      0.00      0.00         1\n",
            "\n",
            "                    accuracy                           0.49        51\n",
            "                   macro avg       0.39      0.45      0.41        51\n",
            "                weighted avg       0.43      0.49      0.45        51\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.4902,\n",
              " 'precision_macro': 0.3929,\n",
              " 'recall_macro': 0.4464,\n",
              " 'f1_macro': 0.4071,\n",
              " 'precision_micro': 0.4902,\n",
              " 'recall_micro': 0.4902,\n",
              " 'f1_micro': 0.4902}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем стекинг лучших моделей, в стандартном варианте не проходило, так как у нас по 1 объекту каждого класса"
      ],
      "metadata": {
        "id": "7cLIku_oWPhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nb_pipeline = make_pipeline(\n",
        "    TfidfVectorizer(),\n",
        "    MultinomialNB()\n",
        ")\n",
        "nb_pipeline.fit(df_train['text'], ytrain)\n",
        "\n",
        "lr_pipeline = make_pipeline(\n",
        "    TfidfVectorizer(ngram_range=(1, 1)),\n",
        "    MaxAbsScaler(),\n",
        "    OneVsRestClassifier(LogisticRegression(solver='liblinear'))\n",
        ")\n",
        "lr_pipeline.fit(df_train['text'], ytrain)\n",
        "\n",
        "bow_pipeline = make_pipeline(\n",
        "    CountVectorizer(ngram_range=(1, 1)),\n",
        "    MaxAbsScaler(),\n",
        "    OneVsRestClassifier(LogisticRegression(solver='liblinear'))\n",
        ")\n",
        "bow_pipeline.fit(df_train['text'], ytrain)\n",
        "\n",
        "stacking_clf = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('nb', nb_pipeline),\n",
        "        ('lr', lr_pipeline),\n",
        "        #('bow', bow_pipeline)\n",
        "    ],\n",
        "    final_estimator=OneVsRestClassifier(LogisticRegression(solver='liblinear')),#OneVsRestClassifier(LogisticRegression(solver='liblinear')\n",
        "    cv='prefit'  # Использование без кросс-валидации, так как модели уже предобучены\n",
        ")\n",
        "\n",
        "stacking_clf.fit(df_train['text'], ytrain)\n",
        "st_pred = stacking_clf.predict(df_test['text'])\n",
        "\n",
        "calculate_multiclass_metrics(ytest.tolist(), st_pred)"
      ],
      "metadata": {
        "id": "YDbH8D7z9O90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим вырастит ли качество, если сложить вероятности классов лучших моделей и поделить их пополам. Качество не менялось"
      ],
      "metadata": {
        "id": "KuhuRPILWpgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_probs = (nb_pred_prob + lr_pred_prob) / 2\n",
        "pred_from_prob = np.argmax(average_probs, axis=1)\n",
        "class_labels = pipeline.named_steps['onevsrestclassifier'].classes_\n",
        "average_pred_classes = [class_labels[index] for index in pred_from_prob]\n",
        "\n",
        "calculate_multiclass_metrics(ytest.tolist(), average_pred_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1dNydt_4sL_",
        "outputId": "c9eb1bd0-e6a4-4f80-ec36-de7078772e15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                               precision    recall  f1-score   support\n",
            "\n",
            "                       Aesop       1.00      1.00      1.00         1\n",
            "             Agatha_Christie       0.00      0.00      0.00         1\n",
            "             Alexandre_Dumas       1.00      1.00      1.00         1\n",
            "             Alphonse_Daudet       1.00      1.00      1.00         1\n",
            "               Anton_Chekhov       1.00      1.00      1.00         1\n",
            "                Aristophanes       1.00      1.00      1.00         1\n",
            "          Arthur_Conan_Doyle       0.00      0.00      0.00         1\n",
            "             Charles_Dickens       0.00      0.00      0.00         1\n",
            "             Dante_Alighieri       0.00      0.00      0.00         1\n",
            "             Edgar_Allan_Poe       1.00      1.00      1.00         1\n",
            "            Ernest_Hemingway       0.00      0.00      0.00         0\n",
            "          F_Scott_Fitzgerald       0.50      1.00      0.67         1\n",
            "                 Franz_Kafka       1.00      1.00      1.00         1\n",
            "          Fyodor_Dostoyevsky       1.00      1.00      1.00         1\n",
            "            Geoffrey_Chaucer       0.50      1.00      0.67         1\n",
            "               George_Orwell       0.00      0.00      0.00         1\n",
            "          Giovanni_Boccaccio       0.50      1.00      0.67         1\n",
            "            Graf_Leo_Tolstoy       0.00      0.00      0.00         1\n",
            "           Guy_De_Maupassant       0.00      0.00      0.00         1\n",
            "               H_P_Lovecraft       0.00      0.00      0.00         1\n",
            "     Hans_Christian_Andersen       1.00      1.00      1.00         1\n",
            "             Haruki_Murakami       1.00      1.00      1.00         1\n",
            "             Herman_Melville       1.00      1.00      1.00         1\n",
            "            Honore_De_Balzac       0.50      1.00      0.67         1\n",
            "                Isaac_Asimov       0.50      1.00      0.67         1\n",
            "               Italo_Calvino       1.00      1.00      1.00         1\n",
            "               Ivan_Turgenev       1.00      1.00      1.00         1\n",
            "               J_R_R_Tolkien       1.00      1.00      1.00         1\n",
            "                 Jack_London       1.00      1.00      1.00         1\n",
            "       James_Fenimore_Cooper       0.50      1.00      0.67         1\n",
            "                 James_Joyce       0.00      0.00      0.00         1\n",
            "                 Jane_Austen       1.00      1.00      1.00         1\n",
            "         Jean_De_La_Fontaine       0.00      0.00      0.00         1\n",
            "  Johann_Wolfgang_Von_Goethe       0.00      0.00      0.00         1\n",
            "                 Jules_Verne       0.50      1.00      0.67         1\n",
            "                 Lev_Tolstoy       0.00      0.00      0.00         0\n",
            "               Marcel_Proust       0.00      0.00      0.00         0\n",
            "                  Mark_Twain       0.00      0.00      0.00         1\n",
            "                 Maxim_Gorky       0.50      1.00      0.67         1\n",
            "                  Mayne_Reid       0.00      0.00      0.00         1\n",
            "Miguel_De_Cervantes_Saavedra       1.00      1.00      1.00         1\n",
            "           Mikhail_Lermontov       0.00      0.00      0.00         0\n",
            "                     Moliere       0.00      0.00      0.00         1\n",
            "              Naguib_Mahfouz       0.00      0.00      0.00         0\n",
            "                     O_Henry       1.00      1.00      1.00         1\n",
            "                 Oscar_Wilde       0.00      0.00      0.00         1\n",
            "      Publius_Vergilius_Maro       0.00      0.00      0.00         1\n",
            "      Robert_Louis_Stevenson       0.00      0.00      0.00         1\n",
            "              Salman_Rushdie       0.00      0.00      0.00         0\n",
            "                   Sophocles       1.00      1.00      1.00         1\n",
            "                    Stendhal       0.00      0.00      0.00         1\n",
            "                Stephen_King       0.00      0.00      0.00         0\n",
            "                 Thomas_Mann       0.00      0.00      0.00         1\n",
            "              Thomas_Pynchon       0.00      0.00      0.00         0\n",
            "                 Umberto_Eco       0.50      1.00      0.67         1\n",
            "                 Victor_Hugo       1.00      1.00      1.00         1\n",
            "              Virginia_Woolf       1.00      1.00      1.00         1\n",
            "            William_Faulkner       0.00      0.00      0.00         1\n",
            "         William_Shakespeare       0.00      0.00      0.00         1\n",
            "\n",
            "                    accuracy                           0.59        51\n",
            "                   macro avg       0.43      0.51      0.46        51\n",
            "                weighted avg       0.50      0.59      0.53        51\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.5882,\n",
              " 'precision_macro': 0.4322,\n",
              " 'recall_macro': 0.5085,\n",
              " 'f1_macro': 0.4576,\n",
              " 'precision_micro': 0.5882,\n",
              " 'recall_micro': 0.5882,\n",
              " 'f1_micro': 0.5882}"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишем код для инференса лучшей модели"
      ],
      "metadata": {
        "id": "5jGmrLWIW7kV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "pipeline = joblib.load('pipeline.pkl')\n",
        "\n",
        "def predict_top_3_authors(text):\n",
        "    text_data = [text]\n",
        "    predicted_probs = pipeline.predict_proba(text_data)\n",
        "    top_3_indices = np.argsort(predicted_probs[0])[-3:][::-1]\n",
        "    top_3_probs = predicted_probs[0][top_3_indices]\n",
        "    top_3_authors = pipeline.named_steps['onevsrestclassifier'].classes_[top_3_indices]\n",
        "    for author, prob in zip(top_3_authors, top_3_probs):\n",
        "        print(f\"Author: {author}, Probability: {prob}\")\n",
        "\n",
        "example_text ='''\n",
        "One ring to rule them all, one ring to find them,\n",
        "One ring to bring them all and in the darkness bind them.'''\n",
        "predict_top_3_authors(example_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WmUx5haSfGE",
        "outputId": "1c49dbf0-29ea-4cf4-ea38-6c432456205a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Author: Antoine_De_Saint_Exupery, Probability: 0.01396337039015265\n",
            "Author: J_R_R_Tolkien, Probability: 0.013169534324720733\n",
            "Author: Aesop, Probability: 0.012547013893284076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее идут ячейки с экспериментами выполненными в ноутбуке на kaggle с большей ОЗУ и с использованием GPU, там мной были полученны вектора W2V, TinyBert, SentanceBert, но модели LR и Catboost на этих признаках по отдельности, вместе и с различными комбинациями с включениями TFidf и вектора на статистиках не показали более высокого результата. Возможно, что результат удастся улучшить, если попробовать дообучить лучшую модель на каких-то векторах из этих экспериментов."
      ],
      "metadata": {
        "id": "1AbhFoKKXFdq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем заменить вектора из словарей на их статистики"
      ],
      "metadata": {
        "id": "nNCJ82dDYic8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_vector_columns(df):\n",
        "    new_columns = {}\n",
        "\n",
        "    for col in df.columns:\n",
        "        # Проверка наличия списка/вектора в каждой ячейке\n",
        "        if df[col].apply(lambda x: isinstance(x, object)).all():\n",
        "            # Вычисление медианных, средних и максимальных значений\n",
        "            new_columns[f'{col}_median'] = df[col].apply(np.median)\n",
        "            new_columns[f'{col}_mean'] = df[col].apply(np.mean)\n",
        "            new_columns[f'{col}_max'] = df[col].apply(np.max)\n",
        "\n",
        "    # Удаляем оригинальные столбцы с векторами\n",
        "    df.drop(columns=[col for col in df.columns if col in new_columns.keys()], inplace=True)\n",
        "\n",
        "    # Добавляем новые столбцы в DataFrame\n",
        "    for new_col, values in new_columns.items():\n",
        "        df[new_col] = values\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "X_train = process_vector_columns(X_train)\n",
        "X_train = X_train.fillna(0)\n",
        "X_test = process_vector_columns(X_test)\n",
        "X_test = X_test.fillna(0)"
      ],
      "metadata": {
        "id": "HCxUxOsyYc9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "\n",
        "model = CatBoostClassifier(\n",
        "    iterations=1000,\n",
        "    learning_rate=0.1,\n",
        "    random_seed=33,\n",
        "    depth=6,\n",
        "    loss_function='MultiClass',\n",
        "    task_type='GPU',\n",
        "    devices='0:1',\n",
        "    verbose=10\n",
        ")\n",
        "\n",
        "model.fit(X_train, ytrain)\n",
        "cb_pred = model.predict(X_test)\n",
        "\n",
        "calculate_multiclass_metrics(ytest.tolist(), cb_pred)"
      ],
      "metadata": {
        "id": "IZcw0vxkY-5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получил эмбеддинги при помощи SentenceTransformer"
      ],
      "metadata": {
        "id": "V1KiQNAsZxXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence-transformers"
      ],
      "metadata": {
        "id": "waAf4pWWZEa9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "train_embeddings = model.encode(df_train['text'].tolist())\n",
        "df_train['embeddings'] = list(train_embeddings)\n",
        "\n",
        "test_embeddings = model.encode(df_test['text'].tolist())\n",
        "df_test['embeddings'] = list(test_embeddings)"
      ],
      "metadata": {
        "id": "alawOOceZEtd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пробовал отобрать наиболее значимые признаки, но не вышло, повисла ячейка"
      ],
      "metadata": {
        "id": "Sr7W4Na8Ziyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Используйте обычную KFold-кросс-валидацию вместо StratifiedKFold\n",
        "cv = KFold(n_splits=3)\n",
        "\n",
        "# Сохранение остальных шагов без изменений\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "rfecv = RFECV(estimator=model, step=1, cv=cv, scoring='f1_weighted', min_features_to_select=1)\n",
        "\n",
        "# Подгонка RFECV\n",
        "rfecv.fit(train_lr, ytrain)\n",
        "\n",
        "# Получение отобранных признаков\n",
        "selected_features = rfecv.support_\n",
        "X_selected = train_lr.loc[:, selected_features]\n",
        "\n",
        "# Вывести выбранные признаки и их количество\n",
        "print(\"Выбранные признаки:\", X_selected.columns.tolist())\n",
        "print(\"Оптимальное количество признаков:\", rfecv.n_features_)"
      ],
      "metadata": {
        "id": "LKYy_JXOZfvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получил эмбеддинги при помощи TinyBERT"
      ],
      "metadata": {
        "id": "dhIsa4_tZ5tO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "model_name = \"huawei-noah/TinyBERT_General_6L_768D\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "\n",
        "def encode_texts(texts):\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1)\n",
        "    return embeddings.numpy()\n",
        "\n",
        "train_embs = encode_texts(df_train['text'].tolist())\n",
        "df_train['tiny_embs'] = list(train_embs)\n",
        "\n",
        "test_embs = encode_texts(df_test['text'].tolist())\n",
        "df_test['tiny_embs'] = list(test_embs)"
      ],
      "metadata": {
        "id": "_VACTwCOZviB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получил эмбеддинги при помощи Word2Vec"
      ],
      "metadata": {
        "id": "lWS-mWKhZ_hJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "def preprocess_texts(texts):\n",
        "    return [text.lower().split() for text in texts]\n",
        "\n",
        "texts = preprocess_texts(df_train['text'])\n",
        "\n",
        "# Обучаем модель Word2Vec\n",
        "w2v_model = Word2Vec(sentences=texts, vector_size=300, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Функция для получения векторного представления текста\n",
        "def text_to_vector(text, model):\n",
        "    words = text.lower().split()\n",
        "    vector = np.mean([model.wv[word] for word in words if word in model.wv], axis=0)\n",
        "    return vector if vector is not np.nan else np.zeros(model.vector_size)\n",
        "\n",
        "df_train['w2v_embs'] = df_train['text'].apply(lambda x: text_to_vector(x, w2v_model))\n",
        "df_train['w2v_embs'] = df_train['w2v_embs'].apply(lambda x: normalize([x])[0])\n",
        "\n",
        "df_test['w2v_embs'] = df_test['text'].apply(lambda x: text_to_vector(x, w2v_model))\n",
        "df_test['w2v_embs'] = df_test['w2v_embs'].apply(lambda x: normalize([x])[0])"
      ],
      "metadata": {
        "id": "MqZN2RROaBoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Собрал в отдельные датасеты все полученные векторы"
      ],
      "metadata": {
        "id": "orS92OlyaUnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vec_train_df['sent_embs'] = df_train['embeddings']\n",
        "vec_test_df['sent_embs'] = df_test['embeddings']\n",
        "vec_train_df['tiny_embs'] = df_train['tiny_embs']\n",
        "vec_test_df['tiny_embs'] = df_test['tiny_embs']\n",
        "vec_train_df['w2v_embs'] = df_train['w2v_embs']\n",
        "vec_test_df['w2v_embs'] = df_test['w2v_embs']\n",
        "vec_train_df['vecs'] = X_train\n",
        "vec_test_df['vecs'] = X_test"
      ],
      "metadata": {
        "id": "N9e-n00KaZNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пробовал подовать модели векторы разбитые на отдельные столбцы"
      ],
      "metadata": {
        "id": "sJ6oifl3alsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_columns = vec_train_df['sent_embs'].apply(pd.Series)\n",
        "new_columns.columns = [f'sent_embs_{i}' for i in range(new_columns.shape[1])]\n",
        "vec_train_df = pd.concat([vec_train_df, new_columns], axis=1)\n",
        "vec_train_df.drop('sent_embs', axis=1, inplace=True)\n",
        "\n",
        "new_columns = vec_train_df['tiny_embs'].apply(pd.Series)\n",
        "new_columns.columns = [f'tiny_embs_{i}' for i in range(new_columns.shape[1])]\n",
        "vec_train_df = pd.concat([vec_train_df, new_columns], axis=1)\n",
        "vec_train_df.drop('tiny_embs', axis=1, inplace=True)\n",
        "\n",
        "new_columns = vec_test_df['sent_embs'].apply(pd.Series)\n",
        "new_columns.columns = [f'sent_embs_{i}' for i in range(new_columns.shape[1])]\n",
        "vec_test_df = pd.concat([vec_test_df, new_columns], axis=1)\n",
        "vec_test_df.drop('sent_embs', axis=1, inplace=True)\n",
        "\n",
        "new_columns = vec_test_df['tiny_embs'].apply(pd.Series)\n",
        "new_columns.columns = [f'tiny_embs_{i}' for i in range(new_columns.shape[1])]\n",
        "vec_test_df = pd.concat([vec_test_df, new_columns], axis=1)\n",
        "vec_test_df.drop('tiny_embs', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "1U0GAM0easDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пробовал объединять все полученные векторы в один"
      ],
      "metadata": {
        "id": "q8pn6vSUa82v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_eval(s):\n",
        "    # Преобразуем в строку, если данные еще не строки\n",
        "    if isinstance(s, str):\n",
        "        cleaned_str = s.replace('\\x00', '')\n",
        "        return eval(cleaned_str)\n",
        "    # Предполагаем, что если не строка, то это уже массив\n",
        "    return s\n",
        "\n",
        "# Применение функции к каждому элементу\n",
        "test['vecs'] = test['vecs'].apply(clean_and_eval)\n",
        "test['sent_embs'] = test['sent_embs'].apply(clean_and_eval)\n",
        "test['tiny_embs'] = test['tiny_embs'].apply(clean_and_eval)\n",
        "test['w2v_embs'] = test['w2v_embs'].apply(clean_and_eval)\n",
        "\n",
        "# Конкатенация эмбеддингов\n",
        "test['combine'] = test.apply(lambda row: np.concatenate([\n",
        "    np.array(row['vecs']),\n",
        "    np.array(row['sent_embs']),\n",
        "    np.array(row['tiny_embs']),\n",
        "    np.array(row['w2v_embs'])\n",
        "]), axis=1)"
      ],
      "metadata": {
        "id": "m-UvpiaJbAaf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}