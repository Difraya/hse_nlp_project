# Введение
Задача заключается в построении базовой модели для предсказания авторства текстовых фрагментов из датасета, содержащего 100 авторов, каждый из которых представлен одной книгой. Это задача многоклассовой классификации, где цель состоит в правильном идентифицировании автора на основе предоставленного текста.
# Мотивация выбора модели
Выбор моделей и метрик обусловлен характером задачи, свойствами датасета и требованиями к реализации линейных моделей и методов, основанных на правилах.
# Предобработка данных<br>
1. **Очистка текста**: Удаление нежелательных символов, HTML-тегов и лишних пробелов.
2. **Токенизация**: Разделение текста на отдельные слова или токены.
3. **Лемматизация и стемминг**: Приведение слов к их базовой форме.
4. **Извлечение признаков**: Преобразование текста в численные данные с использованием таких методов, как bag-of-words (BoW), частота термов - обратная частотность документа (TF-IDF), и частота конкретных частей речи и знаков препинания.
# Метрики для оценки<br>
1. **Точность**: Доля правильно предсказанных авторов от общего числа предсказаний.
2. **Точность, Полнота, F1-Score**: Для общей оценки работы классификатора, особенно для управления дисбалансом классов, если он присутствует.
3. **Матрица ошибок**: Для понимания ошибок классификации между различными авторами.
# Базовые модели и методы<br>
## Эвристические методы, основанные на правилах
1. **Анализ частоты знаков препинания**: Измерение частоты запятых, точек с запятой и других знаков препинания. Авторы часто имеют характерные стили пунктуации.
2. **Частота частей речи**: Анализ распределения частей речи, таких как существительные, глаголы, прилагательные и др. Разные авторы могут проявлять уникальные стилистические предпочтения в письме.
3. **Анализ длины слова и длины предложения**: Выявление характерных паттернов в средней длине слов и предложений.
4. **Богатство словаря**: Измерение разнообразия с использованием таких метрик, как коэффициент тип/токен (TTR) или доля редко встречающихся слов.
## Линейные модели
1. **Логистическая регрессия**: Использование с различными методами регуляризации (L1, L2) для работы с высокими размерностями.
2. **Метод опорных векторов (SVM)**: Линейное ядро для создания надежного бейзлайна из-за его эффективности в высокоразмерных пространствах.
3. **Ридж и лассо регрессия**: Для их способности работать с мультиколлинеарностью и проводить отбор признаков.
4. **Наивный байесовский классификатор**: Хотя не является линейным, он предоставляет хороший вероятностный подход, основанный на правилах, который часто хорошо работает с текстовыми данными.
# Отбор признаков
- **Критерий хи-квадрат**: Для выбора наиболее информативных признаков.
- **Рекурсивное устранение признаков**: Для уменьшения переобучения и улучшения интерпретируемости модели.
# Процесс эксперимента
1. **Разделение на обучающую и валидационную выборки**: Разделение датасета на обучающую и валидационную выборки с учетом стратификации для обеспечения пропорционального представления всех классов.
2. **Кросс-валидация**: Проведение кросс-валидации с K-слоями для обеспечения надежности и достоверности оценки модели.
3. **Настройка гиперпараметров**: Использование поиска по сетке или случайного поиска для оптимизации гиперпараметров для каждой линейной модели.
# Заключение
Этот базовый подход использует как эвристические методы, так и линейные модели для создания надежной отправной точки для атрибуции автора. Комбинируя традиционные линейные классификаторы с инсайтами, основанными на правилах, мы создаем комплексную начальную структуру. Этот бейзлайн может быть улучшен итеративно с использованием более сложных моделей машинного обучения или технологий глубокого обучения в будущих этапах. Выбранные метрики предоставляют подробное и тонкое понимание производительности модели, что важно для уточнения подхода.
